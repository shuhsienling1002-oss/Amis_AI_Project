import streamlit as st
import pandas as pd
import sqlite3
import json
import time
import re
import os
from datetime import datetime
from PIL import Image
import io
import google.generativeai as genai
from github import Github

# ==========================================
# 0. é é¢é…ç½® (ç‰©ç†é–å®šæ¨£å¼)
# ==========================================
st.set_page_config(page_title="'Amis/Pangcah AI", layout="wide", page_icon="ğŸ¦…")

# ==========================================
# ğŸ”’ å®‰å…¨é–˜é–€ (é€™æ˜¯å”¯ä¸€æ–°å¢çš„å€å¡Š)
# ==========================================
# èªªæ˜ï¼šé€™æ®µç¨‹å¼ç¢¼æœƒé˜»æ“‹å¾ŒçºŒæ‰€æœ‰å…§å®¹çš„è¼‰å…¥ï¼Œç›´åˆ°é©—è­‰é€šéã€‚
# é€šéå¾Œï¼Œå®ƒæœƒè¨­å®š session_state.api_keyï¼Œé€™æ¨£æ‚¨åŸæœ¬çš„ç¨‹å¼ç¢¼å°±èƒ½ç›´æ¥æŠ“åˆ° keyã€‚

if "auth_status" not in st.session_state:
    st.session_state.auth_status = False
if "api_key" not in st.session_state:
    st.session_state.api_key = ""

if not st.session_state.auth_status:
    st.title("ğŸ”’ ç³»çµ±é–å®šä¿è­·")
    st.markdown("### 'Amis/Pangcah AI æ ¸å¿ƒç³»çµ±")
    st.info("è«‹è¼¸å…¥ Google API Key ä»¥è§£é™¤é–å®šä¸¦å­˜å–å®Œæ•´åŠŸèƒ½ã€‚")
    
    input_key = st.text_input("API Key", type="password", help="è¼¸å…¥æ‚¨çš„ Gemini API Key")
    
    if st.button("ğŸš€ è§£é–é€²å…¥"):
        if not input_key:
            st.warning("è«‹è¼¸å…¥é‡‘é‘°ã€‚")
        else:
            try:
                # æ¸¬è©¦é‡‘é‘°æ˜¯å¦æœ‰æ•ˆ
                genai.configure(api_key=input_key)
                genai.list_models() 
                
                # é©—è­‰æˆåŠŸï¼šå­˜å…¥ç‹€æ…‹ä¸¦é‡æ•´
                st.session_state.auth_status = True
                st.session_state.api_key = input_key # é€™æœƒè‡ªå‹•å°æ‡‰åˆ°æ‚¨åŸç‰ˆç¨‹å¼ç¢¼çš„ sidebar value
                st.success("âœ… é©—è­‰æˆåŠŸï¼æ­£åœ¨å•Ÿå‹•æ ¸å¿ƒå¼•æ“...")
                time.sleep(1)
                st.rerun()
            except Exception as e:
                st.error(f"âŒ é‡‘é‘°ç„¡æ•ˆæˆ–é€£ç·šå¤±æ•—: {e}")
    
    st.divider()
    st.caption("ğŸ”’ Unauthorized Access Prohibited.")
    st.stop() # â›” ã€é—œéµæŒ‡ä»¤ã€‘é€™è£¡æœƒå¼·åˆ¶åœæ­¢ç¨‹å¼ï¼Œä¿è­·ä¸‹æ–¹ç¨‹å¼ç¢¼ä¸è¢«åŸ·è¡Œ

# ==========================================
# â¬‡ï¸ ä»¥ä¸‹æ˜¯æ‚¨ä¸Šå‚³çš„ 100% åŸç‰ˆç¨‹å¼ç¢¼ (æœªåšä»»ä½•ä¿®æ”¹) â¬‡ï¸
# ==========================================

# ==========================================
# 1. æ ¸å¿ƒå¼•æ“ (ç‰©ç†é–å®š)
# ==========================================

@st.cache_resource(show_spinner=False)
def get_verified_models(api_key):
    """
    è‡ªå‹•åµæ¸¬ä½¿ç”¨è€…å¸³è™Ÿå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œä¸¦å„ªå…ˆæ’åº Flash ç‰ˆæœ¬
    """
    if not api_key: return []
    try:
        genai.configure(api_key=api_key)
        # å–å¾—æ‰€æœ‰æ”¯æ´ generateContent çš„æ¨¡å‹
        ms = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        
        # æ’åºé‚è¼¯ï¼šå„ªå…ˆæ‰¾ 'flash'ï¼Œå…¶æ¬¡æ˜¯ 'pro'
        # é€™æ¨£å¯ä»¥ç¢ºä¿è‡ªå‹•é¸åˆ° gemini-1.5-flash æˆ– gemini-flash-latest ç­‰å­˜åœ¨æ–¼åˆ—è¡¨ä¸­çš„æ¨¡å‹
        ms.sort(key=lambda x: 0 if 'flash' in x else (1 if 'pro' in x else 2))
        
        return ms if ms else ["models/gemini-1.5-flash"]
    except: return ["models/gemini-1.5-flash"]

def run_query(sql, params=(), fetch=False):
    """è³‡æ–™åº«åŸ·è¡Œå¼•æ“"""
    try:
        with sqlite3.connect('amis_data.db', timeout=30) as conn:
            c = conn.cursor()
            c.execute(sql, params)
            if fetch: return c.fetchall()
            conn.commit()
            return True
    except: return [] if fetch else False

def reorder_ids(table):
    """ç‰©ç† ID é˜²æ’é‡ç·¨"""
    rows = run_query(f"SELECT rowid FROM {table} ORDER BY created_at ASC", fetch=True)
    if not rows: return 0
    for idx, (rid,) in enumerate(rows):
        run_query(f"UPDATE {table} SET id = ? WHERE rowid = ?", (idx + 1, rid))
    run_query(f"DELETE FROM sqlite_sequence WHERE name=?", (table,))
    run_query(f"INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", (table, len(rows)))
    return len(rows)

def sync_vocabulary(sentence):
    """è‡ªå‹•å–®å­—åŒæ­¥"""
    words = re.findall(r"\w+", sentence.lower())
    for word in words:
        exists = run_query("SELECT id FROM vocabulary WHERE LOWER(amis) = ?", (word,), fetch=True)
        if not exists:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            run_query("INSERT INTO vocabulary (amis, note, created_at) VALUES (?, ?, ?)", (word, f"ä¾†è‡ªå¥å‹: {sentence}", now))

def is_linguistically_relevant(keyword, target_word):
    """
    [çµ•å°é˜²ç¦¦ç‰ˆ] è©æ³•éæ¿¾å™¨ (2025-12-20 Final Fix)
    """
    k = keyword.lower().strip()
    t = target_word.lower().strip()
    if k == t: return True
    if len(k) == 1: return False 
    if t.startswith(k) or t.endswith(k): return True
    if k in t and len(k) > 2: return True
    return False

# [çµ‚æ¥µä¿®å¾©] å°èˆªç‰ˆé›²ç«¯å‚™ä»½åŠŸèƒ½
def backup_to_github():
    """çµ‚æ¥µå°èˆªç‰ˆï¼šç²¾æº–é€£ç·šå€‰åº«ä¸¦å‚™ä»½"""
    token = st.secrets.get("general", {}).get("GITHUB_TOKEN") or st.secrets.get("GITHUB_TOKEN")
    if not token:
        st.error("âŒ æœªåµæ¸¬åˆ° GitHub Tokenã€‚")
        return False
    try:
        g = Github(token)
        user_name = "shuhsienling1002-oss"
        repo_name = "Amis_AI_Project"
        repo = g.get_user(user_name).get_repo(repo_name)
        file_path = "amis_data.db"
        with open(file_path, "rb") as f:
            content = f.read()
        try:
            contents = repo.get_contents(file_path)
            repo.update_file(contents.path, f"Mobile update: {datetime.now()}", content, contents.sha)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼è³‡æ–™å·²å›å‚³ GitHubã€‚", icon="âœ…")
            return True
        except Exception:
            repo.create_file(file_path, f"Initial DB: {datetime.now()}", content)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼(å·²å»ºç«‹æ–°è³‡æ–™æª”)", icon="âœ…")
            return True
    except Exception as e:
        st.error(f"âš ï¸ é€£ç·šå¤±æ•—ã€‚è«‹ç¢ºèª Token æ¬Šé™ã€‚éŒ¯èª¤: {str(e)}")
        return False

# [è³‡æ–™ç˜¦èº«è¡“] æ”¹ç”¨ç²¾ç°¡æ ¼å¼ï¼Œå¤§å¹…æ¸›å°‘ Token æ¶ˆè€—
def get_full_database_context():
    """
    è®€å–æ•´å€‹è³‡æ–™åº«ï¼Œä½†ä½¿ç”¨ 'è³‡æ–™ç˜¦èº«è¡“' (CSV style) ä¾†ç¯€çœ Tokenã€‚
    è®“ AI å³ä½¿è®€å–å¤§é‡è³‡æ–™ä¹Ÿä¸å®¹æ˜“çˆ†é¡åº¦ã€‚
    """
    ctx = "ã€å…¨é‡é˜¿ç¾èªè³‡æ–™åº« (Compact Mode)ã€‘\n"
    
    # 1. æå–æ‰€æœ‰è©å½™ (ç²¾ç°¡ç‰ˆ)
    # æ ¼å¼ï¼šé˜¿ç¾èª,ä¸­æ–‡,è©æ€§ (å»é™¤å†—è¨€è´…å­—)
    vocab = run_query("SELECT amis, chinese, part_of_speech FROM vocabulary", fetch=True)
    if vocab:
        ctx += "==VOCABULARY==\n"
        for v in vocab:
            # å¦‚æœæ¬„ä½æ˜¯ Noneï¼Œè½‰ç‚ºç©ºå­—ä¸²
            a = v[0] if v[0] else ""
            c = v[1] if v[1] else ""
            p = v[2] if v[2] else ""
            ctx += f"{a},{c},{p}\n"
    
    # 2. æå–æ‰€æœ‰å¥å‹ (ç²¾ç°¡ç‰ˆ)
    sents = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs", fetch=True)
    if sents:
        ctx += "\n==SENTENCES==\n"
        for s in sents:
            sa = s[0] if s[0] else ""
            sc = s[1] if s[1] else ""
            ctx += f"{sa} || {sc}\n"
            
    return ctx

def get_expert_knowledge(query_text, direction="AtoZ"):
    """
    é›™å‘ RAG æª¢ç´¢é‚è¼¯
    """
    if not query_text: return None, [], [], "" 
    clean_q = query_text.strip().rstrip('.?!')
    
    if direction == "AtoZ":
        sql = "SELECT output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(REPLACE(output_sentencepattern_amis, '.', '')) = ? LIMIT 1"
    else:
        sql = "SELECT output_sentencepattern_amis FROM sentence_pairs WHERE LOWER(output_sentencepattern_chinese) = ? LIMIT 1"
    sentence_match = run_query(sql, (clean_q.lower(),), fetch=True)
    full_trans = sentence_match[0][0] if sentence_match else None
    
    query_words = re.findall(r"\w+", query_text.lower())
    words_data, sentences_data, rag_context_parts = [], [], []
    
    try:
        with sqlite3.connect('amis_data.db') as conn:
            for word in query_words:
                matched_definitions = [] 
                should_use_semantic = True
                if len(word) == 1: should_use_semantic = False

                if direction == "AtoZ":
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech FROM vocabulary WHERE LOWER(amis) LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                else:
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech FROM vocabulary WHERE chinese LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                
                valid_vocab_count = 0
                for w in res_vocab:
                    if direction == "AtoZ" and not is_linguistically_relevant(word, w[0]): continue 
                    if valid_vocab_count >= 50: break 
                    words_data.append({"amis": w[0], "chinese": w[1], "pos": w[2]})
                    rag_context_parts.append(f"[é˜¿ç¾èªè³‡æ–™åº«] é˜¿ç¾èª: {w[0]} | ä¸­æ–‡: {w[1]} (è©æ€§: {w[2]})")
                    if w[1] and should_use_semantic: matched_definitions.append(w[1])
                    valid_vocab_count += 1
                
                if direction == "AtoZ":
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(output_sentencepattern_amis) LIKE ? LIMIT 30", (f"%{word}%",), fetch=True)
                else:
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 30", (f"%{word}%",), fetch=True)
                
                res_sent_semantic = []
                if direction == "AtoZ" and matched_definitions and should_use_semantic:
                    for distinct_def in list(set(matched_definitions))[:3]:
                        core_def = distinct_def.split('(')[0].split('ï¼ˆ')[0].strip()
                        if len(core_def) > 0:
                            found = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 20", (f"%{core_def}%",), fetch=True)
                            res_sent_semantic.extend(found)
                
                all_raw_sents = res_sent_direct + res_sent_semantic
                valid_sent_count, processed_sents = 0, set()
                
                for s in all_raw_sents:
                    amis_s, chinese_s = s[0], s[1]
                    if (amis_s, chinese_s) in processed_sents: continue
                    processed_sents.add((amis_s, chinese_s))
                    pass_check = False
                    sent_words = re.findall(r"\w+", amis_s.lower())
                    for sw in sent_words:
                        if is_linguistically_relevant(word, sw): pass_check = True; break
                    if not pass_check and direction == "AtoZ" and should_use_semantic:
                        for distinct_def in list(set(matched_definitions))[:3]:
                             core_def = distinct_def.split('(')[0].split('ï¼ˆ')[0].strip()
                             if core_def and core_def in chinese_s: pass_check = True; break
                    if not pass_check: continue
                    if {"amis": amis_s, "chinese": chinese_s} not in sentences_data:
                        if valid_sent_count >= 20: break
                        sentences_data.append({"amis": amis_s, "chinese": chinese_s})
                        rag_context_parts.append(f"[é˜¿ç¾èªè³‡æ–™åº«] ä¾‹å¥(é˜¿ç¾èª): {amis_s} | (ä¸­æ–‡): {chinese_s}")
                        valid_sent_count += 1
    except: pass
    
    if len(rag_context_parts) > 80:
        rag_context_parts = rag_context_parts[:80]
        rag_context_parts.append("(System: åƒè€ƒè³‡æ–™éå¤šï¼Œå·²æˆªå–å‰ 80 ç­†)")
    rag_prompt = "\nã€é˜¿ç¾èªèªæ–™åº«æª¢ç´¢çµæœ (Amis Corpus)ã€‘:\n" + "\n".join(set(rag_context_parts)) if rag_context_parts else ""
    return full_trans, words_data, sentences_data, rag_prompt

# ==========================================
# 2. ä»‹é¢æ¨¡çµ„ (é‚„åŸæ‰€æœ‰èªªæ˜æ–‡å­—)
# ==========================================

def assistant_system(api_key, model_selection):
    st.title("â— AI æ™ºæ…§ç¿»è­¯æ©Ÿ")
    
    DREAM_MODEL_NAME = "ğŸ§¬ Pangcah/'Amis_language_mode"
    # [æ›´æ–°] åœ¨é€™è£¡å‹•æ…‹ç²å–æ¨¡å‹åˆ—è¡¨
    available_models = get_verified_models(api_key)
    is_pangcah_mode = (model_selection == DREAM_MODEL_NAME)
    
    missing_word_protocol = """
    ã€ç‰¹æ®Šç¿»è­¯æ¨¡å¼ï¼šç¼ºè©æ¨™è¨˜ (Missing Word Protocol)ã€‘
    1. åƒ…é™ä½¿ç”¨æä¾›çš„è³‡æ–™åº«å…§å®¹ã€‚
    2. **é—œéµè¦å‰‡**ï¼šè‹¥ä¸­æ–‡è©å½™ï¼ˆå¦‚åœ°åã€åè©ï¼‰åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°å°æ‡‰é˜¿ç¾èªï¼Œè«‹**ç›´æ¥ä¿ç•™ä¸­æ–‡**ï¼Œä¸è¦è‡ªè¡Œç¿»è­¯æˆ–ç”¨æ‹¼éŸ³ã€‚
    3. è¼¸å‡ºç¯„ä¾‹ï¼šè‹¥ç„¡ 'èŠ±è“®'ï¼Œç¿»è­¯ 'æˆ‘åœ¨èŠ±è“®' -> 'I èŠ±è“® kako'ã€‚
    """
    
    # [æ¨¡å¼åˆ†æµ]
    if is_pangcah_mode:
        # ==========================================
        # æ¨¡å¼ A: Pangcah å…¨åº«åˆ†ææ¨¡å¼
        # ==========================================
        
        # [é—œéµä¿®æ­£] è‡ªå‹•æŒ‘é¸æœ€ä½³çš„ Flash æ¨¡å‹ (Auto-Select)
        # é‚è¼¯ï¼šå¾ available_models è£¡æ‰¾å‡ºåå­—å« 'flash' çš„ï¼Œé¸ç¬¬ä¸€å€‹ã€‚
        # é€™æ¨£ä¸ç®¡ Google å«å®ƒ 'gemini-1.5-flash' é‚„æ˜¯ 'gemini-flash-latest'ï¼Œæˆ‘å€‘éƒ½æŠ“å¾—åˆ°ã€‚
        flash_models = [m for m in available_models if 'flash' in m]
        if flash_models:
            proxy_model = flash_models[0] # è‡ªå‹•é¸ç¬¬ä¸€å€‹ Flash æ¨¡å‹
        else:
            proxy_model = available_models[0] if available_models else "models/gemini-1.5-flash" # å‚™æ¡ˆ
        
        st.info(f"ğŸ¦… **Pangcah æ¨¡å¼ (å…¨åº«æ€ç¶­)**ï¼šæ­£åœ¨ä½¿ç”¨ **{proxy_model}** é€²è¡Œæ·±åº¦åˆ†æã€‚(å·²å•Ÿç”¨è³‡æ–™ç˜¦èº«æŠ€è¡“ä»¥ç¯€çœæµé‡)")
        
        if "pangcah_ready" not in st.session_state: st.session_state.pangcah_ready = False
        if "pangcah_context" not in st.session_state: st.session_state.pangcah_context = ""

        if not st.session_state.pangcah_ready:
            st.markdown("#### 1. æº–å‚™éšæ®µ")
            st.write("è«‹å…ˆè®“æ¨¡å‹é€²è¡Œè³‡æ–™åº«æ·±åº¦æƒæã€‚")
            if st.button("ğŸš€ åŸ·è¡Œ Pangcah è³‡æ–™åˆ†æ (è®€å–å…¨åº«)", type="primary"):
                with st.spinner("æ­£åœ¨é–±è®€ä¸¦å£“ç¸®è³‡æ–™åº«..."):
                    ctx = get_full_database_context()
                    st.session_state.pangcah_context = ctx
                    st.session_state.pangcah_ready = True
                st.rerun()
        
        else:
            st.success("âœ… è³‡æ–™åº«åˆ†æå®Œæˆï¼Pangcah æ¨¡å‹å·²å°±ç·’ã€‚")
            if st.button("ğŸ”„ é‡æ–°åˆ†æè³‡æ–™åº«"):
                st.session_state.pangcah_ready = False
                st.rerun()
            
            st.divider()
            st.markdown("#### 2. æ¸¬è©¦èˆ‡äº’å‹•")
            
            user_input = st.text_area("åœ¨æ­¤è¼¸å…¥æ‚¨è¦ç¿»è­¯æˆ–åˆ†æçš„é˜¿ç¾èª/ä¸­æ–‡å…§å®¹ï¼š", height=150)
            
            if st.button("ğŸ¦… é€å‡ºæ¸¬è©¦ (åŸ·è¡Œç¿»è­¯æˆ–èªæ³•åˆ†æ)", type="primary"):
                if not user_input:
                    st.warning("è«‹è¼¸å…¥å…§å®¹")
                elif not api_key:
                    st.warning("è«‹è¨­å®š Google API Key")
                else:
                    try:
                        with st.spinner(f"Pangcah AI æ­£åœ¨æ€è€ƒ (Core: {proxy_model})..."):
                            genai.configure(api_key=api_key)
                            m = genai.GenerativeModel(proxy_model)
                            
                            formatting_instruction = """
                            ã€æ’ç‰ˆç‰¹åˆ¥æŒ‡ä»¤ (Visual Formatting)ã€‘
                            1. ä½¿ç”¨ `### ğŸ¦… é˜¿ç¾èªç¿»è­¯` ä½œç‚ºå°æ¨™é¡Œã€‚
                            2. **é—œéµç¿»è­¯å¥å­**ï¼šè«‹ä½¿ç”¨ `#` (H1) åŠ ä¸Š `:blue[...]` (è—è‰²) å°‡æ•´å¥åŒ…èµ·ä¾†ï¼Œä½¿å…¶æœ€å¤§æœ€é¡¯çœ¼ã€‚
                            3. ç¯„ä¾‹ï¼š
                               ### ğŸ¦… é˜¿ç¾èªç¿»è­¯
                               # :blue[I èŠ±è“® kako.]
                               
                               ### ğŸ“Š èªæ³•åˆ†æ
                               ...
                            """
                            
                            full_prompt = f"{st.session_state.pangcah_context}\n\n{missing_word_protocol}\n\n{formatting_instruction}\n\nã€æŒ‡ä»¤ã€‘\nä½ ç¾åœ¨æ˜¯ Pangcah/'Amis åŸç”Ÿèªè¨€æ¨¡å‹ã€‚å·²é–±è®€ä¸Šæ–¹ã€å…¨é‡è³‡æ–™åº«(Compact)ã€‘ã€‚\nè«‹å°ä½¿ç”¨è€…è¼¸å…¥é€²è¡Œç²¾ç¢ºç¿»è­¯èˆ‡åˆ†æã€‚\nè‹¥è³‡æ–™åº«ç„¡æ­¤è©ï¼Œè«‹ä¿ç•™ä¸­æ–‡ã€‚\n\nä½¿ç”¨è€…è¼¸å…¥: {user_input}"
                            
                            # è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
                            try:
                                response = m.generate_content(full_prompt)
                            except Exception as e:
                                if "429" in str(e):
                                    st.toast("â³ æµé‡èª¿ç¯€ä¸­ (429)ï¼Œç³»çµ±ä¼‘æ¯ 10 ç§’å¾Œè‡ªå‹•é‡è©¦...", icon="ğŸ›¡ï¸")
                                    time.sleep(10)
                                    response = m.generate_content(full_prompt)
                                else:
                                    raise e

                            if response:
                                st.markdown("### ğŸ¦… Pangcah æ¨¡å‹åˆ†æçµæœï¼š")
                                st.write(response.text)
                    except Exception as e: st.error(f"AI éŒ¯èª¤ï¼š{e}")

    else:
        # ==========================================
        # æ¨¡å¼ B: æ¨™æº– RAG æ¨¡å¼
        # ==========================================
        actual_model = model_selection
        mode = st.radio("ç¿»è­¯æ–¹å‘", ["é˜¿ç¾èª â®• ä¸­æ–‡", "ä¸­æ–‡ â®• é˜¿ç¾èª"], horizontal=True)
        direction = "AtoZ" if mode == "é˜¿ç¾èª â®• ä¸­æ–‡" else "ZtoA"
        if "rag_result" not in st.session_state: st.session_state.rag_result = None
        if "last_query" not in st.session_state: st.session_state.last_query = ""
        
        st.subheader("è¼¸å…¥æ–‡å­—")
        with st.form("translation_search"):
            q = st.text_area(f"åœ¨æ­¤è¼¸å…¥å¥å­", height=150)
            submit_search = st.form_submit_button("ğŸš€ 1. æŸ¥è©¢èªæ–™åº«", type="primary")
        if submit_search and q:
            f, w, s, r = get_expert_knowledge(q, direction)
            st.session_state.rag_result = (f, w, s, r)
            st.session_state.last_query = q
        st.divider()
        if st.session_state.rag_result:
            f, w, s, r = st.session_state.rag_result
            if f: st.success(f"### ğŸ† å°ˆå®¶ç¿»è­¯ï¼š\n**{f}**")
            if w:
                with st.expander(f"ğŸ“š ç›¸é—œå–®è© ({len(w)} ç­†)", expanded=True):
                    for item in w: st.markdown(f"- **{item['amis']}** â®• {item['chinese']} ({item['pos']})")
            if s:
                with st.expander(f"ğŸ—£ï¸ ç›¸é—œä¾‹å¥ ({len(s)} ç­†)", expanded=True):
                    for item in s: st.markdown(f"> **{item['amis']}**\n> ({item['chinese']})")
            st.divider()
            st.markdown("### ğŸ¤– AI å”åŒåˆ†æ")
            if st.button("ğŸ¦… åŸ·è¡Œ AI èªæ³•åˆ†æ"):
                if not api_key: st.warning("è«‹è¨­å®š API Key")
                else:
                    try:
                        with st.spinner(f"æ­£åœ¨å‘¼å« {actual_model} ..."):
                            genai.configure(api_key=api_key)
                            m = genai.GenerativeModel(actual_model)
                            final_prompt = f"{r}\n\n{missing_word_protocol}\n\nè«‹æ ¹æ“šä»¥ä¸Šæä¾›çš„ã€é˜¿ç¾èªèªæ–™åº«ã€‘(Amis Corpus)ï¼Œå°ä»¥ä¸‹å¥å­é€²è¡Œè©³ç´°èªæ³•èˆ‡èªæ„åˆ†æã€‚\nè‹¥é‡åˆ°è³‡æ–™åº«æ²’æœ‰çš„è©ï¼Œè«‹ä¾æ“šã€ç¼ºè©æ¨™è¨˜å”è­°ã€‘ä¿ç•™ä¸­æ–‡ã€‚\n\nä½¿ç”¨è€…è¼¸å…¥: {st.session_state.last_query}"
                            response = m.generate_content(final_prompt)
                            if response:
                                st.markdown("#### ğŸ¦… AI åˆ†æå ±å‘Šï¼š")
                                st.write(response.text)
                    except Exception as e: st.error(f"âš ï¸ AI éŒ¯èª¤ï¼š{e}")

# ==========================================
# 3. ä¸»æ§å°
# ==========================================

def main():
    with sqlite3.connect('amis_data.db') as conn:
        conn.execute('CREATE TABLE IF NOT EXISTS sentence_pairs (id INTEGER PRIMARY KEY AUTOINCREMENT, created_at TIMESTAMP, output_sentencepattern_amis TEXT, output_sentencepattern_chinese TEXT, output_sentencepattern_english TEXT)')
        conn.execute('CREATE TABLE IF NOT EXISTS vocabulary (id INTEGER PRIMARY KEY AUTOINCREMENT, amis TEXT, chinese TEXT, english TEXT, part_of_speech TEXT, note TEXT, created_at TIMESTAMP)')
        conn.execute('CREATE TABLE IF NOT EXISTS pos_tags (tag_name TEXT PRIMARY KEY, sort_order INTEGER DEFAULT 0)')
    st.sidebar.title("ğŸ¦… ç³»çµ±é¸å–®")
    
    # [æ–°å¢] è³‡æ–™åº«æ•‘æ´ä¸­å¿ƒ
    with st.sidebar.expander("ğŸ“‚ è³‡æ–™åº«æ•‘æ´ä¸­å¿ƒ", expanded=True):
        st.warning("âš ï¸ è­¦å‘Šï¼šè‹¥é›²ç«¯è³‡æ–™éºå¤±ï¼Œè«‹åœ¨æ­¤ä¸Šå‚³æœ¬æ©Ÿå‚™ä»½æª” (.db) é€²è¡Œé‚„åŸã€‚")
        uploaded_db = st.file_uploader("ä¸Šå‚³ amis_data.db", type=["db"])
        if uploaded_db is not None:
            if st.button("ğŸš¨ ç¢ºèªè¦†è“‹ä¸¦é‚„åŸè³‡æ–™åº«"):
                with open("amis_data.db", "wb") as f:
                    f.write(uploaded_db.getbuffer())
                st.success("âœ… è³‡æ–™åº«é‚„åŸæˆåŠŸï¼è«‹é‡æ–°æ•´ç†é é¢ã€‚")
                time.sleep(2)
                st.rerun()

    with st.sidebar.container():
        st.info("â˜ï¸ **è¡Œå‹•åŒæ­¥ä¸­å¿ƒ**")
        if st.sidebar.button("ğŸ”„ ç«‹å³å°‡è³‡æ–™å‚™ä»½å› GitHub", type="primary"):
            backup_to_github()
    
    # [æ–°å¢] å˜—è©¦å¾ Secrets è®€å– GOOGLE_API_KEY
    default_key = st.secrets.get("GOOGLE_API_KEY", "")
    key = st.sidebar.text_input("Google API Key", type="password", value=st.session_state.get("api_key", default_key))
    
    if key != st.session_state.get("api_key"): 
        st.session_state["api_key"] = key; st.cache_resource.clear(); st.rerun()
    
    # é€™è£¡æœƒè‡ªå‹•å» Google æŸ¥è©¢å¯ç”¨çš„æ¨¡å‹ï¼Œæ‰€ä»¥ä¸ç®¡å«ä»€éº¼åå­—éƒ½èƒ½æŠ“åˆ°
    raw_ms = get_verified_models(key)
    ms = []
    if raw_ms:
        ms = raw_ms.copy()
        DREAM_MODEL = "ğŸ§¬ Pangcah/'Amis_language_mode"
        ms.insert(0, DREAM_MODEL)
    model = st.sidebar.selectbox("è«‹é¸æ“‡ AI æ¨¡å‹", ms, index=0) if ms else None
    
    st.sidebar.divider()
    page = st.sidebar.radio("åŠŸèƒ½æ¨¡å¼", ["ğŸ  ç³»çµ±é¦–é ", "â— AI æ™ºæ…§åŠ©ç†", "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«", "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†", "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†", "ğŸ“ èªæ–™åŒ¯å‡º"])
    
    if page == "ğŸ  ç³»çµ±é¦–é ":
        st.markdown("<h1 style='text-align: center; font-size: 5rem;'>ğŸ¦…</h1>", unsafe_allow_html=True)
        st.markdown("<h1 style='text-align: center;'>'Amis / Pangcah AI</h1>", unsafe_allow_html=True)
        st.divider()
        st.markdown("<p style='text-align: center;'>æ­¡è¿å›ä¾†ï¼Œèˆ¹é•·ã€‚ç³»çµ±å·²å°±ç·’ã€‚</p>", unsafe_allow_html=True)

    elif page == "â— AI æ™ºæ…§åŠ©ç†": assistant_system(key, model)

    elif page == "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«":
        st.title("ğŸ” å°ˆå®¶å¥å‹è³‡æ–™åº«")
        with st.form("add_new_s"):
            c1, c2, c3 = st.columns(3); a, c, e = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡"), c3.text_input("è‹±èª")
            if st.form_submit_button("â• å„²å­˜æ–°å¥å‹"):
                if a and c: 
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO sentence_pairs (output_sentencepattern_amis, output_sentencepattern_chinese, output_sentencepattern_english, created_at) VALUES (?,?,?,?)", (a, c, e, now))
                    sync_vocabulary(a); reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs ORDER BY id DESC", conn)
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic", hide_index=True)
        if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
            with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('sentence_pairs', conn, if_exists='replace', index=False)
            reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()

    elif page == "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†":
        st.title("ğŸ“– å–®è©èªæ–™åº«ç®¡ç†")
        raw_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
        with st.form("add_new_vocab"):
            c1, c2, c4 = st.columns([2, 2, 3])
            a_in, c_in = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡")
            p_in = c4.selectbox("è©é¡", options=raw_tags)
            if st.form_submit_button("â• å„²å­˜æ–°å–®è©"):
                if a_in:
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO vocabulary (amis, chinese, part_of_speech, created_at) VALUES (?,?,?,?)", (a_in, c_in, p_in, now))
                    reorder_ids("vocabulary"); backup_to_github(); st.rerun()
        st.divider()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM vocabulary ORDER BY id DESC", conn)
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic",
            column_config={"part_of_speech": st.column_config.SelectboxColumn("è©é¡ (æœå°‹é¸å–®)", options=raw_tags, required=True)})
        if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
            with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('vocabulary', conn, if_exists='replace', index=False)
            reorder_ids("vocabulary"); backup_to_github(); st.rerun()

    elif page == "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†":
        st.title("ğŸ·ï¸ æ¨™ç±¤ç®¡ç† (Tag Alignment)")
        
        # [æ™ºæ…§æ›´åå·¥å…·]
        with st.expander("âš¡ æ™ºæ…§æ›´åå·¥å…· (é€£å‹•æ›´æ–°å–®è©)", expanded=True):
            current_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
            c1, c2 = st.columns(2)
            old_tag = c1.selectbox("é¸æ“‡è¦ä¿®æ”¹çš„èˆŠæ¨™ç±¤", options=current_tags)
            new_tag_name = c2.text_input("è¼¸å…¥æ–°åç¨±")
            if st.button("ğŸ”„ åŸ·è¡Œæ›´åèˆ‡é€£å‹•æ›´æ–°"):
                if old_tag and new_tag_name and old_tag != new_tag_name:
                    try:
                        with sqlite3.connect('amis_data.db') as conn:
                            conn.execute("UPDATE vocabulary SET part_of_speech = ? WHERE part_of_speech = ?", (new_tag_name, old_tag))
                            conn.execute("INSERT OR IGNORE INTO pos_tags (tag_name) VALUES (?)", (new_tag_name,))
                            conn.execute("DELETE FROM pos_tags WHERE tag_name = ?", (old_tag,))
                        st.success(f"âœ… æˆåŠŸå°‡ '{old_tag}' æ›´åç‚º '{new_tag_name}'ï¼Œä¸¦æ›´æ–°äº†ç›¸é—œå–®è©ï¼")
                        backup_to_github(); time.sleep(1.5); st.rerun()
                    except Exception as e: st.error(f"æ›´æ–°å¤±æ•—: {e}")
        st.divider()

        # [æ–°å¢æ¨™ç±¤]
        with st.form("t"):
            nt = st.text_input("æ–°å¢æ¨™ç±¤åç¨±")
            if st.form_submit_button("æ–°å¢"): 
                run_query("INSERT OR REPLACE INTO pos_tags (tag_name) VALUES (?)", (nt,)) 
                backup_to_github(); st.rerun()

        # ==========================================
        # ğŸ”¥ è‡ªé©æ‡‰æ–°å¢ã€Œå‚™è¨»ã€æ¬„ä½ + æ’åºæ§åˆ¶
        # ==========================================
        with sqlite3.connect('amis_data.db') as conn: 
            df_tags = pd.read_sql("SELECT * FROM pos_tags", conn)

        # 1. è‡ªé©æ‡‰çµæ§‹ï¼šè£œä¸Š description æ¬„ä½
        if "description" not in df_tags.columns:
            df_tags["description"] = "" 

        # 2. æ¬„ä½æ’åºï¼šç¢ºä¿å‚™è¨»åœ¨å³å´
        cols_order = ["tag_name", "description", "sort_order"]
        existing_cols = [c for c in cols_order if c in df_tags.columns]
        remaining_cols = [c for c in df_tags.columns if c not in existing_cols]
        df_tags = df_tags[existing_cols + remaining_cols]

        # 3. ç·¨è¼¯å™¨é…ç½®
        et = st.data_editor(
            df_tags, 
            use_container_width=True, 
            num_rows="dynamic",
            column_config={
                "tag_name": st.column_config.TextColumn("èªæ³•æ¨™ç±¤åç¨±", disabled=True), 
                "description": st.column_config.TextColumn(
                    "å‚™è¨» (LLM å®šç¾©æ ¡æº–)", 
                    help="åœ¨æ­¤èªªæ˜æ­¤æ¨™ç±¤èˆ‡å¤§èªè¨€æ¨¡å‹é€šç”¨å®šç¾©çš„å·®ç•°",
                    width="large" 
                ),
                "sort_order": st.column_config.NumberColumn("æ’åºæ¬Šé‡")
            }
        )

        if st.button("ğŸ’¾ å„²å­˜æ¨™ç±¤èˆ‡å‚™è¨»"):
            with sqlite3.connect('amis_data.db') as conn: 
                et.to_sql('pos_tags', conn, if_exists='replace', index=False)
            backup_to_github(); st.success("å·²å­˜æª”ï¼è³‡æ–™åº«çµæ§‹å·²è‡ªå‹•æ›´æ–°ã€‚"); st.rerun()

    elif page == "ğŸ“ èªæ–™åŒ¯å‡º":
        st.title("ğŸ“ èªæ–™åŒ¯å‡ºèˆ‡æˆ°ç•¥é€²åº¦")
        with st.container():
            st.info("ğŸ—ºï¸ **AI æˆ°ç•¥ç™¼å±•è·¯ç·šåœ– (Roadmap)**")
            c1, c2, c3 = st.columns(3)
            with c1: st.markdown("### ğŸš© ç¬¬ä¸€éšæ®µ (ç›®å‰)"); st.caption("RAG æª¢ç´¢å¢å¼·ç”Ÿæˆ"); st.write("âœ… **Python æ¡ç¤¦æ©Ÿ**\nâœ… **Gemini å»šå¸«**\nğŸ› ï¸ **ç›®æ¨™**ï¼šæŒçºŒæ“´å……èªæ–™åº«ã€‚")
            with c2: st.markdown("### ğŸ”ï¸ ç¬¬äºŒéšæ®µ (1,000+)"); st.caption("å¾®èª¿ (Fine-tuning)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šåˆæ­¥å»ºç«‹å°ˆå±¬æ¨¡å‹ã€‚")
            with c3: st.markdown("### åŸå ¡ğŸ° ç¬¬ä¸‰éšæ®µ (10,000+)"); st.caption("åŸç”Ÿæ¨¡å‹ (Native LLM)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šé˜¿ç¾èªåŸç”Ÿæ¨ç†èƒ½åŠ›ã€‚")
        st.divider()
        tab1, tab2 = st.tabs(["ğŸ“ å¥å‹", "ğŸ“– å–®è©"])
        with tab1:
            with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs", conn)
            st.dataframe(df, use_container_width=True)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df.to_json(orient="records", lines=True, force_ascii=False), "amis_sentences.jsonl")
        with tab2:
            with sqlite3.connect('amis_data.db') as conn: df_v = pd.read_sql("SELECT * FROM vocabulary", conn)
            st.dataframe(df_v, use_container_width=True)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df_v.to_json(orient="records", lines=True, force_ascii=False), "amis_vocabulary.jsonl")

if __name__ == "__main__": main()
