import streamlit as st
import pandas as pd
import sqlite3
import json
import time
import re
from datetime import datetime
from PIL import Image
import io
import google.generativeai as genai
from github import Github

# ==========================================
# 0. é é¢é…ç½® (ç‰©ç†é–å®šæ¨£å¼)
# ==========================================
st.set_page_config(page_title="'Amis/Pangcah AI", layout="wide", page_icon="ğŸ¦…")

# ==========================================
# 1. æ ¸å¿ƒå¼•æ“ (ç‰©ç†é–å®š)
# ==========================================

@st.cache_resource(show_spinner=False)
def get_verified_models(api_key):
    if not api_key: return []
    try:
        genai.configure(api_key=api_key)
        ms = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        ms.sort(key=lambda x: 0 if 'flash' in x else 1)
        return ms if ms else ["models/gemini-1.5-flash-latest"]
    except: return ["models/gemini-1.5-flash-latest"]

def run_query(sql, params=(), fetch=False):
    """è³‡æ–™åº«åŸ·è¡Œå¼•æ“"""
    try:
        with sqlite3.connect('amis_data.db', timeout=30) as conn:
            c = conn.cursor()
            c.execute(sql, params)
            if fetch: return c.fetchall()
            conn.commit()
            return True
    except: return [] if fetch else False

def reorder_ids(table):
    """ç‰©ç† ID é˜²æ’é‡ç·¨"""
    rows = run_query(f"SELECT rowid FROM {table} ORDER BY created_at ASC", fetch=True)
    if not rows: return 0
    for idx, (rid,) in enumerate(rows):
        run_query(f"UPDATE {table} SET id = ? WHERE rowid = ?", (idx + 1, rid))
    run_query(f"DELETE FROM sqlite_sequence WHERE name=?", (table,))
    run_query(f"INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", (table, len(rows)))
    return len(rows)

def sync_vocabulary(sentence):
    """è‡ªå‹•å–®å­—åŒæ­¥"""
    words = re.findall(r"\w+", sentence.lower())
    for word in words:
        exists = run_query("SELECT id FROM vocabulary WHERE LOWER(amis) = ?", (word,), fetch=True)
        if not exists:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            run_query("INSERT INTO vocabulary (amis, note, created_at) VALUES (?, ?, ?)", (word, f"ä¾†è‡ªå¥å‹: {sentence}", now))

def is_linguistically_relevant(keyword, target_word):
    """è©æ³•éæ¿¾å™¨"""
    k = keyword.lower()
    t = target_word.lower()
    if k == t: return True
    if t.startswith(k) or t.endswith(k): return True
    if k in t:
        if len(k) > 3: return True
        else: return False 
    return False

# [çµ‚æ¥µä¿®å¾©] å°èˆªç‰ˆé›²ç«¯å‚™ä»½åŠŸèƒ½ - ç¢ºä¿é€£ç·š shuhsienling1002-oss/Amis_AI_Project
def backup_to_github():
    """çµ‚æ¥µå°èˆªç‰ˆï¼šç²¾æº–é€£ç·šå€‰åº«ä¸¦å‚™ä»½"""
    token = st.secrets.get("general", {}).get("GITHUB_TOKEN") or st.secrets.get("GITHUB_TOKEN")
    if not token:
        st.error("âŒ æœªåµæ¸¬åˆ° GitHub Tokenã€‚")
        return False
    try:
        g = Github(token)
        user_name = "shuhsienling1002-oss"
        repo_name = "Amis_AI_Project"
        repo = g.get_user(user_name).get_repo(repo_name)
        file_path = "amis_data.db"
        with open(file_path, "rb") as f:
            content = f.read()
        try:
            contents = repo.get_contents(file_path)
            repo.update_file(contents.path, f"Mobile update: {datetime.now()}", content, contents.sha)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼è³‡æ–™å·²å›å‚³ GitHubã€‚", icon="âœ…")
            return True
        except Exception:
            repo.create_file(file_path, f"Initial DB: {datetime.now()}", content)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼(å·²å»ºç«‹æ–°è³‡æ–™æª”)", icon="âœ…")
            return True
    except Exception as e:
        st.error(f"âš ï¸ é€£ç·šå¤±æ•—ã€‚è«‹ç¢ºèª Token æ¬Šé™ã€‚éŒ¯èª¤: {str(e)}")
        return False

def get_expert_knowledge(query_text, direction="AtoZ"):
    """é›™å‘ RAG æª¢ç´¢é‚è¼¯"""
    if not query_text: return None, [], [], "" 
    clean_q = query_text.strip().rstrip('.?!')
    if direction == "AtoZ":
        sql = "SELECT output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(REPLACE(output_sentencepattern_amis, '.', '')) = ? LIMIT 1"
    else:
        sql = "SELECT output_sentencepattern_amis FROM sentence_pairs WHERE LOWER(output_sentencepattern_chinese) = ? LIMIT 1"
    sentence_match = run_query(sql, (clean_q.lower(),), fetch=True)
    full_trans = sentence_match[0][0] if sentence_match else None
    query_words = re.findall(r"\w+", query_text.lower())
    words_data, sentences_data, rag_context_parts = [], [], []
    try:
        with sqlite3.connect('amis_data.db') as conn:
            for word in query_words:
                matched_definitions = [] 
                if direction == "AtoZ":
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech FROM vocabulary WHERE LOWER(amis) LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                else:
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech FROM vocabulary WHERE chinese LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                valid_vocab_count = 0
                for w in res_vocab:
                    if direction == "AtoZ" and not is_linguistically_relevant(word, w[0]): continue 
                    if valid_vocab_count >= 50: break 
                    words_data.append({"amis": w[0], "chinese": w[1], "pos": w[2]})
                    rag_context_parts.append(f"[é˜¿ç¾èªè³‡æ–™åº«] é˜¿ç¾èª: {w[0]} | ä¸­æ–‡: {w[1]} (è©æ€§: {w[2]})")
                    if w[1]: matched_definitions.append(w[1])
                    valid_vocab_count += 1
                if direction == "AtoZ":
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(output_sentencepattern_amis) LIKE ? LIMIT 30", (f"%{word}%",), fetch=True)
                else:
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 30", (f"%{word}%",), fetch=True)
                res_sent_semantic = []
                if direction == "AtoZ" and matched_definitions:
                    for distinct_def in list(set(matched_definitions))[:3]:
                        core_def = distinct_def.split('(')[0].split('ï¼ˆ')[0].strip()
                        if len(core_def) > 0:
                            found = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 20", (f"%{core_def}%",), fetch=True)
                            res_sent_semantic.extend(found)
                all_raw_sents = res_sent_direct + res_sent_semantic
                valid_sent_count, processed_sents = 0, set()
                for s in all_raw_sents:
                    amis_s, chinese_s = s[0], s[1]
                    if (amis_s, chinese_s) in processed_sents: continue
                    processed_sents.add((amis_s, chinese_s))
                    pass_check = False
                    sent_words = re.findall(r"\w+", amis_s.lower())
                    for sw in sent_words:
                        if is_linguistically_relevant(word, sw): pass_check = True; break
                    if not pass_check and direction == "AtoZ":
                        for distinct_def in list(set(matched_definitions))[:3]:
                             core_def = distinct_def.split('(')[0].split('ï¼ˆ')[0].strip()
                             if core_def and core_def in chinese_s: pass_check = True; break
                    if not pass_check: continue
                    if {"amis": amis_s, "chinese": chinese_s} not in sentences_data:
                        if valid_sent_count >= 20: break
                        sentences_data.append({"amis": amis_s, "chinese": chinese_s})
                        rag_context_parts.append(f"[é˜¿ç¾èªè³‡æ–™åº«] ä¾‹å¥(é˜¿ç¾èª): {amis_s} | (ä¸­æ–‡): {chinese_s}")
                        valid_sent_count += 1
    except: pass
    if len(rag_context_parts) > 80:
        rag_context_parts = rag_context_parts[:80]
        rag_context_parts.append("(System: åƒè€ƒè³‡æ–™éå¤šï¼Œå·²æˆªå–å‰ 80 ç­†)")
    rag_prompt = "\nã€é˜¿ç¾èªèªæ–™åº«æª¢ç´¢çµæœ (Amis Corpus)ã€‘:\n" + "\n".join(set(rag_context_parts)) if rag_context_parts else ""
    return full_trans, words_data, sentences_data, rag_prompt

# ==========================================
# 2. ä»‹é¢æ¨¡çµ„ (é‚„åŸæ‰€æœ‰èªªæ˜æ–‡å­—)
# ==========================================

def assistant_system(api_key, model_selection):
    st.title("â— AI æ™ºæ…§ç¿»è­¯æ©Ÿ")
    DREAM_MODEL_NAME = "ğŸ§¬ Pangcah/'Amis-language-model (ç›®æ¨™æ§‹å»ºä¸­)"
    available_models = get_verified_models(api_key)
    if model_selection == DREAM_MODEL_NAME:
        proxy_model = "models/gemini-1.5-flash-latest" 
        real_models = [m for m in available_models if "Pangcah" not in m]
        if real_models: proxy_model = real_models[0] 
        st.info(f"ğŸ¦… **ç›®æ¨™é–å®š**ï¼šæ‚¨é¸æ“‡äº†æœªä¾†çš„ Pangcah æ¨¡å‹ï¼ç›®å‰ç³»çµ±å°‡ç”± **{proxy_model}** ä»£ç†åŸ·è¡Œã€‚")
        actual_model = proxy_model
    else:
        actual_model = model_selection
    mode = st.radio("ç¿»è­¯æ–¹å‘", ["é˜¿ç¾èª â®• ä¸­æ–‡", "ä¸­æ–‡ â®• é˜¿ç¾èª"], horizontal=True)
    direction = "AtoZ" if mode == "é˜¿ç¾èª â®• ä¸­æ–‡" else "ZtoA"
    if "rag_result" not in st.session_state: st.session_state.rag_result = None
    if "last_query" not in st.session_state: st.session_state.last_query = ""
    st.subheader("è¼¸å…¥æ–‡å­—")
    with st.form("translation_search"):
        q = st.text_area(f"åœ¨æ­¤è¼¸å…¥å¥å­", height=150)
        submit_search = st.form_submit_button("ğŸš€ 1. æŸ¥è©¢èªæ–™åº«", type="primary")
    if submit_search and q:
        f, w, s, r = get_expert_knowledge(q, direction)
        st.session_state.rag_result = (f, w, s, r)
        st.session_state.last_query = q
    st.divider()
    if st.session_state.rag_result:
        f, w, s, r = st.session_state.rag_result
        if f: st.success(f"### ğŸ† å°ˆå®¶ç¿»è­¯ï¼š\n**{f}**")
        if w:
            with st.expander(f"ğŸ“š ç›¸é—œå–®è© ({len(w)} ç­†)", expanded=True):
                for item in w: st.markdown(f"- **{item['amis']}** â®• {item['chinese']} ({item['pos']})")
        if s:
            with st.expander(f"ğŸ—£ï¸ ç›¸é—œä¾‹å¥ ({len(s)} ç­†)", expanded=True):
                for item in s: st.markdown(f"> **{item['amis']}**\n> ({item['chinese']})")
        st.divider()
        st.markdown("### ğŸ¤– AI å”åŒåˆ†æ")
        if st.button("ğŸ¦… åŸ·è¡Œ AI èªæ³•åˆ†æ"):
            if not api_key: st.warning("è«‹è¨­å®š API Key")
            else:
                try:
                    with st.spinner(f"æ­£åœ¨å‘¼å« {actual_model} ..."):
                        genai.configure(api_key=api_key)
                        m = genai.GenerativeModel(actual_model)
                        final_prompt = f"{r}\n\nè«‹æ ¹æ“šä»¥ä¸Šæä¾›çš„ã€é˜¿ç¾èªèªæ–™åº«ã€‘(Amis Corpus)ï¼Œå°ä»¥ä¸‹å¥å­é€²è¡Œè©³ç´°èªæ³•èˆ‡èªæ„åˆ†æ: {st.session_state.last_query}"
                        response = m.generate_content(final_prompt)
                        if response:
                            st.markdown("#### ğŸ¦… AI åˆ†æå ±å‘Šï¼š")
                            st.write(response.text)
                except Exception as e: st.error(f"âš ï¸ AI éŒ¯èª¤ï¼š{e}")

# ==========================================
# 3. ä¸»æ§å°
# ==========================================

def main():
    with sqlite3.connect('amis_data.db') as conn:
        conn.execute('CREATE TABLE IF NOT EXISTS sentence_pairs (id INTEGER PRIMARY KEY AUTOINCREMENT, created_at TIMESTAMP, output_sentencepattern_amis TEXT, output_sentencepattern_chinese TEXT, output_sentencepattern_english TEXT)')
        conn.execute('CREATE TABLE IF NOT EXISTS vocabulary (id INTEGER PRIMARY KEY AUTOINCREMENT, amis TEXT, chinese TEXT, english TEXT, part_of_speech TEXT, note TEXT, created_at TIMESTAMP)')
        conn.execute('CREATE TABLE IF NOT EXISTS pos_tags (tag_name TEXT PRIMARY KEY, sort_order INTEGER DEFAULT 0)')
    st.sidebar.title("ğŸ¦… ç³»çµ±é¸å–®")
    with st.sidebar.container():
        st.info("â˜ï¸ **è¡Œå‹•åŒæ­¥ä¸­å¿ƒ**")
        if st.sidebar.button("ğŸ”„ ç«‹å³å°‡è³‡æ–™å‚™ä»½å› GitHub", type="primary"):
            backup_to_github()
    with st.sidebar.expander("ğŸ”§ è³‡æ–™åº«æ•´å½¢è¨ºæ‰€"):
        if st.button("ğŸ› ï¸ 1. åŸ·è¡Œï¼šå¥å‹åº«é‡æ§‹"):
            try:
                with sqlite3.connect('amis_data.db') as conn:
                    conn.execute("ALTER TABLE sentence_pairs RENAME TO sentence_pairs_old_backup")
                    conn.execute('CREATE TABLE sentence_pairs (id INTEGER PRIMARY KEY AUTOINCREMENT, created_at TIMESTAMP, output_sentencepattern_amis TEXT, output_sentencepattern_chinese TEXT, output_sentencepattern_english TEXT)')
                    conn.execute("""INSERT INTO sentence_pairs (output_sentencepattern_amis, output_sentencepattern_chinese, output_sentencepattern_english, created_at) SELECT output_sentencepattern_amis, output_sentencepattern_chinese, output_sentencepattern_english, created_at FROM sentence_pairs_old_backup""")
                    conn.execute("DROP TABLE sentence_pairs_old_backup")
                    reorder_ids("sentence_pairs")
                st.sidebar.success("âœ… ä¿®å¾©å®Œæˆï¼"); time.sleep(1); st.rerun()
            except Exception as e: st.sidebar.error(f"éŒ¯èª¤: {e}")
    key = st.sidebar.text_input("Google API Key", type="password", value=st.session_state.get("api_key", ""))
    if key != st.session_state.get("api_key"): 
        st.session_state["api_key"] = key; st.cache_resource.clear(); st.rerun()
    raw_ms = get_verified_models(key)
    ms = []
    if raw_ms:
        ms = raw_ms.copy()
        DREAM_MODEL = "ğŸ§¬ Pangcah/'Amis-language-model (ç›®æ¨™æ§‹å»ºä¸­)"
        ms.insert(0, DREAM_MODEL)
    model = st.sidebar.selectbox("è«‹é¸æ“‡ AI æ¨¡å‹", ms, index=0) if ms else None
    st.sidebar.divider()
    page = st.sidebar.radio("åŠŸèƒ½æ¨¡å¼", ["ğŸ  ç³»çµ±é¦–é ", "â— AI æ™ºæ…§åŠ©ç†", "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«", "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†", "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†", "ğŸ“ èªæ–™åŒ¯å‡º"])
    
    if page == "ğŸ  ç³»çµ±é¦–é ":
        st.markdown("<h1 style='text-align: center; font-size: 5rem;'>ğŸ¦…</h1>", unsafe_allow_html=True)
        st.markdown("<h1 style='text-align: center;'>'Amis / Pangcah AI</h1>", unsafe_allow_html=True)
        st.divider()
        st.markdown("<p style='text-align: center;'>æ­¡è¿å›ä¾†ï¼Œèˆ¹é•·ã€‚ç³»çµ±å·²å°±ç·’ã€‚</p>", unsafe_allow_html=True)

    elif page == "â— AI æ™ºæ…§åŠ©ç†": assistant_system(key, model)

    elif page == "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«":
        st.title("ğŸ” å°ˆå®¶å¥å‹è³‡æ–™åº«")
        with st.form("add_new_s"):
            c1, c2, c3 = st.columns(3); a, c, e = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡"), c3.text_input("è‹±èª")
            if st.form_submit_button("â• å„²å­˜æ–°å¥å‹"):
                if a and c: 
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO sentence_pairs (output_sentencepattern_amis, output_sentencepattern_chinese, output_sentencepattern_english, created_at) VALUES (?,?,?,?)", (a, c, e, now))
                    sync_vocabulary(a); reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs ORDER BY id DESC", conn)
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic", hide_index=True)
        if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
            with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('sentence_pairs', conn, if_exists='replace', index=False)
            reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()

    elif page == "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†":
        st.title("ğŸ“– å–®è©èªæ–™åº«ç®¡ç†")
        raw_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
        with st.form("add_new_vocab"):
            c1, c2, c4 = st.columns([2, 2, 3])
            a_in, c_in = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡")
            p_in = c4.selectbox("è©é¡", options=raw_tags)
            if st.form_submit_button("â• å„²å­˜æ–°å–®è©"):
                if a_in:
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO vocabulary (amis, chinese, part_of_speech, created_at) VALUES (?,?,?,?)", (a_in, c_in, p_in, now))
                    reorder_ids("vocabulary"); backup_to_github(); st.rerun()
        st.divider()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM vocabulary ORDER BY id DESC", conn)
        # [æ¢å¾©é¸å–®] å¸¶æœ‰æœå°‹åŠŸèƒ½çš„æ ¼å­é¸å–®
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic",
            column_config={"part_of_speech": st.column_config.SelectboxColumn("è©é¡ (æœå°‹é¸å–®)", options=raw_tags, required=True)})
        if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
            with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('vocabulary', conn, if_exists='replace', index=False)
            reorder_ids("vocabulary"); backup_to_github(); st.rerun()

    elif page == "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†":
        st.title("ğŸ·ï¸ æ¨™ç±¤ç®¡ç† (Tag Alignment)")
        
        # [æ™ºæ…§æ›´åå·¥å…·] (é€£å‹•é‚è¼¯ä¿æŒä¸è®Š)
        with st.expander("âš¡ æ™ºæ…§æ›´åå·¥å…· (é€£å‹•æ›´æ–°å–®è©)", expanded=True):
            current_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
            c1, c2 = st.columns(2)
            old_tag = c1.selectbox("é¸æ“‡è¦ä¿®æ”¹çš„èˆŠæ¨™ç±¤", options=current_tags)
            new_tag_name = c2.text_input("è¼¸å…¥æ–°åç¨±")
            if st.button("ğŸ”„ åŸ·è¡Œæ›´åèˆ‡é€£å‹•æ›´æ–°"):
                if old_tag and new_tag_name and old_tag != new_tag_name:
                    try:
                        with sqlite3.connect('amis_data.db') as conn:
                            conn.execute("UPDATE vocabulary SET part_of_speech = ? WHERE part_of_speech = ?", (new_tag_name, old_tag))
                            conn.execute("INSERT OR IGNORE INTO pos_tags (tag_name) VALUES (?)", (new_tag_name,))
                            conn.execute("DELETE FROM pos_tags WHERE tag_name = ?", (old_tag,))
                        st.success(f"âœ… æˆåŠŸå°‡ '{old_tag}' æ›´åç‚º '{new_tag_name}'ï¼Œä¸¦æ›´æ–°äº†ç›¸é—œå–®è©ï¼")
                        backup_to_github(); time.sleep(1.5); st.rerun()
                    except Exception as e: st.error(f"æ›´æ–°å¤±æ•—: {e}")
        st.divider()

        # [æ–°å¢æ¨™ç±¤]
        with st.form("t"):
            nt = st.text_input("æ–°å¢æ¨™ç±¤åç¨±")
            if st.form_submit_button("æ–°å¢"): 
                # ç°¡å–®æ’å…¥ï¼Œè‹¥ç„¡ description æ¬„ä½å‰‡è‡ªå‹•è™•ç†ï¼Œå¾…ä¸‹æ–¹ç·¨è¼¯å™¨é–‹å•Ÿæ™‚æœƒè‡ªå‹•è£œä¸Šçµæ§‹
                run_query("INSERT OR REPLACE INTO pos_tags (tag_name) VALUES (?)", (nt,)) 
                backup_to_github(); st.rerun()

        # ==========================================
        # ğŸ”¥ é‡é»ä¿®æ”¹å€ï¼šè‡ªé©æ‡‰æ–°å¢ã€Œå‚™è¨»ã€æ¬„ä½
        # ==========================================
        with sqlite3.connect('amis_data.db') as conn: 
            df_tags = pd.read_sql("SELECT * FROM pos_tags", conn)

        # 1. è‡ªé©æ‡‰çµæ§‹ï¼šå¦‚æœè³‡æ–™åº«è£¡æ²’æœ‰ description æ¬„ä½ï¼Œæˆ‘å€‘åœ¨è¨˜æ†¶é«”ä¸­è‡ªå‹•åŠ ä¸Š
        if "description" not in df_tags.columns:
            df_tags["description"] = "" # é è¨­ç‚ºç©ºå­—ä¸²

        # 2. æ¬„ä½æ’åºï¼šç¢ºä¿ tag_name åœ¨å‰ï¼Œdescription åœ¨æ‚¨è¦çš„å³å´
        # å¦‚æœæœ‰ sort_orderï¼Œæˆ‘å€‘ä¿ç•™å®ƒï¼Œç†æƒ³é †åº: tag_name -> description -> sort_order
        cols_order = ["tag_name", "description", "sort_order"]
        existing_cols = [c for c in cols_order if c in df_tags.columns]
        # æŠŠå…¶ä»–æ²’åˆ—åœ¨ä¸Šé¢çš„æ¬„ä½ä¹ŸåŠ å›ä¾† (é˜²ç¦¦æ€§ç¨‹å¼ç¢¼)
        remaining_cols = [c for c in df_tags.columns if c not in existing_cols]
        df_tags = df_tags[existing_cols + remaining_cols]

        # 3. ç·¨è¼¯å™¨é…ç½®ï¼šè¨­å®šæ¬„ä½æ¨™é¡Œèˆ‡å¯¬åº¦
        et = st.data_editor(
            df_tags, 
            use_container_width=True, 
            num_rows="dynamic",
            column_config={
                "tag_name": st.column_config.TextColumn("èªæ³•æ¨™ç±¤åç¨±", disabled=True), # é–å®šä¸»éµä¸è®“æ”¹ï¼Œé¿å…è³‡æ–™åº«éŒ¯äº‚
                "description": st.column_config.TextColumn(
                    "å‚™è¨» (LLM å®šç¾©æ ¡æº–)", 
                    help="åœ¨æ­¤èªªæ˜æ­¤æ¨™ç±¤èˆ‡å¤§èªè¨€æ¨¡å‹é€šç”¨å®šç¾©çš„å·®ç•°",
                    width="large" # åŠ å¯¬æ­¤æ¬„ä½ä»¥ä¾¿è¼¸å…¥
                ),
                "sort_order": st.column_config.NumberColumn("æ’åºæ¬Šé‡")
            }
        )

        if st.button("ğŸ’¾ å„²å­˜æ¨™ç±¤èˆ‡å‚™è¨»"):
            with sqlite3.connect('amis_data.db') as conn: 
                # ä½¿ç”¨ replace æ¨¡å¼ï¼Œé€™æœƒè‡ªå‹•æ ¹æ“šæ–°çš„ DataFrame çµæ§‹é‡å»ºè³‡æ–™è¡¨ (åŒ…å«æ–°åŠ çš„å‚™è¨»æ¬„ä½)
                et.to_sql('pos_tags', conn, if_exists='replace', index=False)
            backup_to_github(); st.success("å·²å­˜æª”ï¼è³‡æ–™åº«çµæ§‹å·²è‡ªå‹•æ›´æ–°ã€‚"); st.rerun()

    elif page == "ğŸ“ èªæ–™åŒ¯å‡º":
        st.title("ğŸ“ èªæ–™åŒ¯å‡ºèˆ‡æˆ°ç•¥é€²åº¦")
        # [é‚„åŸ] AI æˆ°ç•¥ç™¼å±•è·¯ç·šåœ–
        with st.container():
            st.info("ğŸ—ºï¸ **AI æˆ°ç•¥ç™¼å±•è·¯ç·šåœ– (Roadmap)**")
            c1, c2, c3 = st.columns(3)
            with c1: st.markdown("### ğŸš© ç¬¬ä¸€éšæ®µ (ç›®å‰)"); st.caption("RAG æª¢ç´¢å¢å¼·ç”Ÿæˆ"); st.write("âœ… **Python æ¡ç¤¦æ©Ÿ**\nâœ… **Gemini å»šå¸«**\nğŸ› ï¸ **ç›®æ¨™**ï¼šæŒçºŒæ“´å……èªæ–™åº«ã€‚")
            with c2: st.markdown("### ğŸ”ï¸ ç¬¬äºŒéšæ®µ (1,000+)"); st.caption("å¾®èª¿ (Fine-tuning)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šåˆæ­¥å»ºç«‹å°ˆå±¬æ¨¡å‹ã€‚")
            with c3: st.markdown("### åŸå ¡ğŸ° ç¬¬ä¸‰éšæ®µ (10,000+)"); st.caption("åŸç”Ÿæ¨¡å‹ (Native LLM)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šé˜¿ç¾èªåŸç”Ÿæ¨ç†èƒ½åŠ›ã€‚")
        st.divider()
        tab1, tab2 = st.tabs(["ğŸ“ å¥å‹", "ğŸ“– å–®è©"])
        with tab1:
            with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs", conn)
            st.dataframe(df, use_container_width=True)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df.to_json(orient="records", lines=True, force_ascii=False), "amis_sentences.jsonl")
        with tab2:
            with sqlite3.connect('amis_data.db') as conn: df_v = pd.read_sql("SELECT * FROM vocabulary", conn)
            st.dataframe(df_v, use_container_width=True)
            st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df_v.to_json(orient="records", lines=True, force_ascii=False), "amis_vocabulary.jsonl")

if __name__ == "__main__": main()
