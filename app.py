import streamlit as st
import pandas as pd
import sqlite3
import json
import time
import re
import os
from datetime import datetime
from PIL import Image
import io
import google.generativeai as genai
from github import Github

# ==========================================
# 0. é é¢é…ç½® (ç‰©ç†é–å®šæ¨£å¼)
# ==========================================
st.set_page_config(page_title="'Amis/Pangcah AI", layout="wide", page_icon="ğŸ¦…")

# ==========================================
# ğŸ”’ å®‰å…¨é–˜é–€
# ==========================================
if "auth_status" not in st.session_state:
    st.session_state.auth_status = False
if "api_key" not in st.session_state:
    st.session_state.api_key = ""

if not st.session_state.auth_status:
    st.title("ğŸ”’ ç³»çµ±é–å®šä¿è­·")
    st.markdown("### 'Amis/Pangcah AI æ ¸å¿ƒç³»çµ±")
    st.info("è«‹è¼¸å…¥ç³»çµ±å¯†ç¢¼ä»¥è§£é™¤é–å®šä¸¦å­˜å–å®Œæ•´åŠŸèƒ½ã€‚")
    
    input_key = st.text_input("ç³»çµ±å¯†ç¢¼", type="password", help="è«‹è¼¸å…¥è¨ªå•å¯†ç¢¼")
    
    if st.button("ğŸš€ è§£é–é€²å…¥"):
        if input_key == "836489":
            st.session_state.auth_status = True
            # è§£é–å¾Œå…ˆå˜—è©¦è¼‰å…¥ secrets ä¸­çš„ keyï¼Œè‹¥ç„¡å‰‡ç•™ç©ºè®“å´é‚Šæ¬„è™•ç†
            st.session_state.api_key = st.secrets.get("GOOGLE_API_KEY", "")
            st.success("âœ… é©—è­‰æˆåŠŸï¼æ­£åœ¨å•Ÿå‹•æ ¸å¿ƒå¼•æ“...")
            time.sleep(1)
            st.rerun()
        else:
            st.error("âŒ å¯†ç¢¼éŒ¯èª¤ï¼Œæ‹’çµ•å­˜å–ã€‚")
    st.divider()
    st.caption("ğŸ”’ Unauthorized Access Prohibited.")
    st.stop() 

# ==========================================
# 1. æ ¸å¿ƒå¼•æ“ (ç‰©ç†é–å®š)
# ==========================================

@st.cache_resource(show_spinner=False)
def get_verified_models(api_key):
    if not api_key: return []
    try:
        genai.configure(api_key=api_key)
        ms = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        ms.sort(key=lambda x: 0 if 'flash' in x else (1 if 'pro' in x else 2))
        return ms if ms else ["models/gemini-1.5-flash"]
    except: return ["models/gemini-1.5-flash"]

def run_query(sql, params=(), fetch=False):
    try:
        with sqlite3.connect('amis_data.db', timeout=30) as conn:
            c = conn.cursor()
            c.execute(sql, params)
            if fetch: return c.fetchall()
            conn.commit()
            return True
    except: return [] if fetch else False

def reorder_ids(table):
    rows = run_query(f"SELECT rowid FROM {table} ORDER BY created_at ASC", fetch=True)
    if not rows: return 0
    for idx, (rid,) in enumerate(rows):
        run_query(f"UPDATE {table} SET id = ? WHERE rowid = ?", (idx + 1, rid))
    run_query(f"DELETE FROM sqlite_sequence WHERE name=?", (table,))
    run_query(f"INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", (table, len(rows)))
    return len(rows)

def sync_vocabulary(sentence):
    words = re.findall(r"\w+", sentence.lower())
    for word in words:
        exists = run_query("SELECT id FROM vocabulary WHERE LOWER(amis) = ?", (word,), fetch=True)
        if not exists:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            run_query("INSERT INTO vocabulary (amis, note, created_at) VALUES (?, ?, ?)", (word, f"ä¾†è‡ªå¥å‹: {sentence}", now))

def is_linguistically_relevant(keyword, target_word):
    k = keyword.lower().strip()
    t = target_word.lower().strip()
    if k == t: return True
    if len(k) == 1: return False 
    if t.startswith(k) or t.endswith(k): return True
    if k in t and len(k) > 2: return True
    return False

def backup_to_github():
    token = st.secrets.get("general", {}).get("GITHUB_TOKEN") or st.secrets.get("GITHUB_TOKEN")
    if not token:
        st.error("âŒ æœªåµæ¸¬åˆ° GitHub Tokenã€‚")
        return False
    try:
        g = Github(token)
        user_name = "shuhsienling1002-oss"
        repo_name = "Amis_AI_Project"
        repo = g.get_user(user_name).get_repo(repo_name)
        file_path = "amis_data.db"
        with open(file_path, "rb") as f:
            content = f.read()
        try:
            contents = repo.get_contents(file_path)
            repo.update_file(contents.path, f"Mobile update: {datetime.now()}", content, contents.sha)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼è³‡æ–™å·²å›å‚³ GitHubã€‚", icon="âœ…")
            return True
        except Exception:
            repo.create_file(file_path, f"Initial DB: {datetime.now()}", content)
            st.toast("â˜ï¸ é›²ç«¯å‚™ä»½æˆåŠŸï¼(å·²å»ºç«‹æ–°è³‡æ–™æª”)", icon="âœ…")
            return True
    except Exception as e:
        st.error(f"âš ï¸ é€£ç·šå¤±æ•—ã€‚è«‹ç¢ºèª Token æ¬Šé™ã€‚éŒ¯èª¤: {str(e)}")
        return False

# ==========================================
# æ ¸å¿ƒä¿®æ”¹å€ï¼šè³‡æ–™è®€å–å„ªåŒ– (å£“ç¸® + Note)
# ==========================================

def get_full_database_context():
    """
    ã€Layer 2 å„ªåŒ–ï¼šæ¥µé™å£“ç¸®æ¨¡å¼ã€‘
    ç‚ºäº†é¿å… 429 Quota Exceededï¼Œæˆ‘å€‘å°‡è³‡æ–™æ ¼å¼å£“ç¸®ç‚ºé¡ CSV æ ¼å¼ã€‚
    æ ¼å¼å®šç¾©ï¼š
    å–®è©å€ï¼šAmis,Chinese,POS|Note
    å¥å‹å€ï¼šAmis||Chinese|Note
    """
    ctx = "Dataset:Amis-Note-Compressed\n"
    
    # 1. è®€å–å–®è© (å« Note)
    vocab = run_query("SELECT amis, chinese, part_of_speech, note FROM vocabulary", fetch=True)
    if vocab:
        ctx += "==V==\n" # V = Vocabulary
        for v in vocab:
            a = v[0] if v[0] else ""
            c = v[1] if v[1] else ""
            p = v[2] if v[2] else ""
            n = v[3] if v[3] else ""
            
            # å£“ç¸®é‚è¼¯ï¼šè‹¥ç„¡ noteï¼Œçœå»åˆ†éš”ç¬¦
            line = f"{a},{c},{p}"
            if n:
                line += f"|{n}"
            ctx += line + "\n"
                
    # 2. è®€å–å¥å‹ (å« Note)
    sents = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese, note FROM sentence_pairs", fetch=True)
    if sents:
        ctx += "==S==\n" # S = Sentences
        for s in sents:
            sa = s[0] if s[0] else ""
            sc = s[1] if s[1] else ""
            sn = s[2] if s[2] else ""
            
            # å£“ç¸®é‚è¼¯
            line = f"{sa}||{sc}"
            if sn:
                line += f"|{sn}"
            ctx += line + "\n"
            
    return ctx

def get_expert_knowledge(query_text, direction="AtoZ"):
    """
    ã€æ¨™æº– RAG æ¨¡å¼ã€‘
    é€™è£¡ä¹Ÿå¿…é ˆåŠ å…¥ Note çš„è®€å–ï¼Œè®“ä¸€èˆ¬æŸ¥è©¢ä¹Ÿèƒ½çœ‹åˆ°å‚™è¨»ã€‚
    """
    if not query_text: return None, [], [], "" 
    clean_q = query_text.strip().rstrip('.?!')
    if direction == "AtoZ":
        sql = "SELECT output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(REPLACE(output_sentencepattern_amis, '.', '')) = ? LIMIT 1"
    else:
        sql = "SELECT output_sentencepattern_amis FROM sentence_pairs WHERE LOWER(output_sentencepattern_chinese) = ? LIMIT 1"
    sentence_match = run_query(sql, (clean_q.lower(),), fetch=True)
    full_trans = sentence_match[0][0] if sentence_match else None
    
    query_words = re.findall(r"\w+", query_text.lower())
    words_data, sentences_data, rag_context_parts = [], [], []
    try:
        with sqlite3.connect('amis_data.db') as conn:
            for word in query_words:
                matched_definitions = [] 
                should_use_semantic = True
                if len(word) == 1: should_use_semantic = False
                
                # ä¿®æ”¹ï¼šSQL åŠ å…¥ note
                if direction == "AtoZ":
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech, note FROM vocabulary WHERE LOWER(amis) LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                else:
                    res_vocab = run_query("SELECT amis, chinese, part_of_speech, note FROM vocabulary WHERE chinese LIKE ? LIMIT 100", (f"%{word}%",), fetch=True)
                
                valid_vocab_count = 0
                for w in res_vocab:
                    if direction == "AtoZ" and not is_linguistically_relevant(word, w[0]): continue 
                    if valid_vocab_count >= 50: break 
                    
                    note_content = w[3] if w[3] else ""
                    words_data.append({"amis": w[0], "chinese": w[1], "pos": w[2]})
                    
                    # æç¤ºè©åŒ…å«å‚™è¨»
                    rag_str = f"[å–®è©] {w[0]} : {w[1]} ({w[2]})"
                    if note_content:
                        rag_str += f" [å‚™è¨»: {note_content}]"
                    rag_context_parts.append(rag_str)
                    
                    if w[1] and should_use_semantic: matched_definitions.append(w[1])
                    if note_content and should_use_semantic: matched_definitions.append(note_content)
                    valid_vocab_count += 1
                
                # å¥å‹æª¢ç´¢ (ç¶­æŒåŸæ¨£ï¼Œä½†å¢åŠ æ•¸é‡é™åˆ¶ä»¥é˜²çˆ†æ‰)
                if direction == "AtoZ":
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE LOWER(output_sentencepattern_amis) LIKE ? LIMIT 20", (f"%{word}%",), fetch=True)
                else:
                    res_sent_direct = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 20", (f"%{word}%",), fetch=True)
                
                res_sent_semantic = []
                # ... (èªæ„æœå°‹é‚è¼¯) ...
                if direction == "AtoZ" and matched_definitions and should_use_semantic:
                    for distinct_def in list(set(matched_definitions))[:2]: # é™åˆ¶èªæ„æœå°‹æ¬¡æ•¸
                        core_def = distinct_def.split('(')[0].split('ï¼ˆ')[0].strip()
                        if len(core_def) > 0:
                            found = run_query("SELECT output_sentencepattern_amis, output_sentencepattern_chinese FROM sentence_pairs WHERE output_sentencepattern_chinese LIKE ? LIMIT 10", (f"%{core_def}%",), fetch=True)
                            res_sent_semantic.extend(found)
                            
                all_raw_sents = res_sent_direct + res_sent_semantic
                valid_sent_count, processed_sents = 0, set()
                for s in all_raw_sents:
                    amis_s, chinese_s = s[0], s[1]
                    if (amis_s, chinese_s) in processed_sents: continue
                    processed_sents.add((amis_s, chinese_s))
                    # ... (ç›¸é—œæ€§æª¢æŸ¥é‚è¼¯ç•¥ï¼Œä¿æŒç°¡æ½”) ...
                    
                    # ç›´æ¥åŠ å…¥
                    if {"amis": amis_s, "chinese": chinese_s} not in sentences_data:
                        if valid_sent_count >= 15: break
                        sentences_data.append({"amis": amis_s, "chinese": chinese_s})
                        rag_context_parts.append(f"[ä¾‹å¥] {amis_s} || {chinese_s}")
                        valid_sent_count += 1
    except: pass
    
    # RAG çµæœæˆªæ–·ä¿è­·
    if len(rag_context_parts) > 60:
        rag_context_parts = rag_context_parts[:60]
        rag_context_parts.append("(System: åƒè€ƒè³‡æ–™éå¤šï¼Œå·²æ™ºæ…§æˆªå–)")
    rag_prompt = "\nã€æª¢ç´¢çµæœ (RAG)ã€‘:\n" + "\n".join(set(rag_context_parts)) if rag_context_parts else ""
    return full_trans, words_data, sentences_data, rag_prompt

# ==========================================
# 2. ä»‹é¢æ¨¡çµ„ (åŒ…å« 429 éŒ¯èª¤è™•ç†)
# ==========================================

def assistant_system(api_key, model_selection):
    st.title("â— AI æ™ºæ…§ç¿»è­¯æ©Ÿ")
    DREAM_MODEL_NAME = "ğŸ§¬ Pangcah/'Amis_language_mode"
    available_models = get_verified_models(api_key)
    is_pangcah_mode = (model_selection == DREAM_MODEL_NAME)
    
    missing_word_protocol = """
    ã€ç‰¹æ®Šå”è­°ã€‘
    1. åƒ…é™ä½¿ç”¨æä¾›çš„è³‡æ–™åº«ã€‚
    2. è³‡æ–™æ ¼å¼ç‚ºå£“ç¸®ç‰ˆï¼š
       - å–®è©å€ (==V==): é˜¿ç¾èª,ä¸­æ–‡,è©æ€§|å‚™è¨»
       - å¥å‹å€ (==S==): é˜¿ç¾èª||ä¸­æ–‡|å‚™è¨»
    3. è‹¥ç„¡å°æ‡‰è©ï¼Œè«‹ä¿ç•™åŸæ–‡ã€‚
    """
    
    if is_pangcah_mode:
        flash_models = [m for m in available_models if 'flash' in m]
        if flash_models: proxy_model = flash_models[0]
        else: proxy_model = available_models[0] if available_models else "models/gemini-1.5-flash"
        
        st.info(f"ğŸ¦… **Pangcah æ¨¡å¼ (å…¨åº«æ€ç¶­)**ï¼šæ­£åœ¨ä½¿ç”¨ **{proxy_model}**ã€‚(å·²å•Ÿç”¨æ¥µé™è³‡æ–™å£“ç¸®æŠ€è¡“)")
        
        if "pangcah_ready" not in st.session_state: st.session_state.pangcah_ready = False
        if "pangcah_context" not in st.session_state: st.session_state.pangcah_context = ""
        if "last_translation" not in st.session_state: st.session_state.last_translation = ""
        if "last_input_text" not in st.session_state: st.session_state.last_input_text = ""

        if not st.session_state.pangcah_ready:
            st.markdown("#### 1. æº–å‚™éšæ®µ")
            st.write("è«‹å…ˆè®“æ¨¡å‹é€²è¡Œè³‡æ–™åº«æ·±åº¦æƒæã€‚")
            if st.button("ğŸš€ åŸ·è¡Œ Pangcah è³‡æ–™åˆ†æ (è®€å–å…¨åº«)", type="primary"):
                with st.spinner("æ­£åœ¨é–±è®€ä¸¦å£“ç¸®è³‡æ–™åº«..."):
                    ctx = get_full_database_context()
                    st.session_state.pangcah_context = ctx
                    st.session_state.pangcah_ready = True
                st.rerun()
        else:
            st.success("âœ… è³‡æ–™åº«åˆ†æå®Œæˆï¼Pangcah æ¨¡å‹å·²å°±ç·’ã€‚")
            if st.button("ğŸ”„ é‡æ–°åˆ†æè³‡æ–™åº« (æ–°å¢è³‡æ–™å¾Œè«‹æŒ‰æ­¤)"):
                st.session_state.pangcah_ready = False
                st.rerun()
            
            st.divider()
            st.markdown("#### 2. æ¸¬è©¦èˆ‡äº’å‹•")
            
            user_input = st.text_area("åœ¨æ­¤è¼¸å…¥æ‚¨è¦ç¿»è­¯æˆ–åˆ†æçš„é˜¿ç¾èª/ä¸­æ–‡å…§å®¹ï¼š", height=150)
            
            # --- ç¿»è­¯æŒ‰éˆ• (å« Error Handling) ---
            if st.button("ğŸ¦… åŸ·è¡Œç¿»è­¯ (ä¸å«åˆ†æ)", type="primary"):
                if not user_input:
                    st.warning("è«‹è¼¸å…¥å…§å®¹")
                elif not api_key:
                    st.warning("è«‹è¨­å®š Google API Key")
                else:
                    try:
                        with st.spinner(f"Pangcah AI æ­£åœ¨ç¿»è­¯ (Core: {proxy_model})..."):
                            genai.configure(api_key=api_key)
                            m = genai.GenerativeModel(proxy_model)
                            formatting_instruction = """
                            ã€æ’ç‰ˆæŒ‡ä»¤ã€‘
                            1. ä½¿ç”¨ `### ğŸ¦… ç¿»è­¯çµæœ` ä½œç‚ºæ¨™é¡Œã€‚
                            2. é—œéµå¥è«‹ç”¨ `### :blue[...]` åŒ…è£¹ã€‚
                            3. è«‹åƒè€ƒè³‡æ–™åº«ä¸­çš„ 'å‚™è¨»' (|Note) ä¾†å¢å¼·ç¿»è­¯æº–ç¢ºåº¦ï¼Œä½†ä¸ä¸€å®šè¦é¡¯ç¤ºå‡ºä¾†ã€‚
                            """
                            full_prompt = f"{st.session_state.pangcah_context}\n\n{missing_word_protocol}\n\n{formatting_instruction}\n\nä½¿ç”¨è€…è¼¸å…¥: {user_input}"
                            
                            try:
                                response = m.generate_content(full_prompt)
                            except Exception as e:
                                # 429 éŒ¯èª¤è™•ç†ï¼šè‡ªå‹•å†·å» 60 ç§’
                                if "429" in str(e):
                                    wait_time = 60
                                    st.toast(f"â³ æµé‡æ»¿è¼‰ (429)ï¼Œç³»çµ±è‡ªå‹•å†·å» {wait_time} ç§’...", icon="ğŸ§Š")
                                    with st.spinner(f"å¼•æ“é™æº«ä¸­... è«‹ç¨å€™ {wait_time} ç§’"):
                                        time.sleep(wait_time)
                                    response = m.generate_content(full_prompt)
                                else:
                                    raise e

                            if response:
                                st.session_state.last_translation = response.text
                                st.session_state.last_input_text = user_input
                    except Exception as e: st.error(f"AI éŒ¯èª¤ï¼š{e}")

            if st.session_state.last_translation:
                st.markdown("---")
                st.write(st.session_state.last_translation)
                
                st.markdown("#### ğŸ§  é€²éšæŒ‡ä»¤")
                
                # --- å°è©±æŒ‰éˆ• (å« Error Handling) ---
                if st.button("ğŸ’¬ æ¨¡æ“¬å°è©±å›æ‡‰", use_container_width=True):
                    try:
                        with st.spinner("Pangcah AI æ­£åœ¨æ€è€ƒå›æ‡‰..."):
                            genai.configure(api_key=api_key)
                            m = genai.GenerativeModel(proxy_model)
                            chat_prompt = f"""
                            {st.session_state.pangcah_context}
                            ã€æŒ‡ä»¤ã€‘
                            ä½¿ç”¨è€…: "{st.session_state.last_input_text}"
                            æ„æ€: "{st.session_state.last_translation}"
                            è«‹æ‰®æ¼”é˜¿ç¾æ—è€†è€(Faki/Fayi)ç”¨é˜¿ç¾èªå›æ‡‰(é™„ä¸­æ–‡)ã€‚
                            æ’ç‰ˆï¼šé˜¿ç¾èªè«‹ç”¨ `###` åŠ å¤§ã€‚
                            """
                            try:
                                response_chat = m.generate_content(chat_prompt)
                            except Exception as e:
                                if "429" in str(e):
                                    wait_time = 60
                                    st.toast(f"â³ æµé‡æ»¿è¼‰ (429)ï¼Œç³»çµ±è‡ªå‹•å†·å» {wait_time} ç§’...", icon="ğŸ§Š")
                                    with st.spinner(f"å¼•æ“é™æº«ä¸­... è«‹ç¨å€™ {wait_time} ç§’"):
                                        time.sleep(wait_time)
                                    response_chat = m.generate_content(chat_prompt)
                                else:
                                    raise e

                            if response_chat:
                                st.markdown("### ğŸ’¬ AI å°è©±å›æ‡‰ï¼š")
                                st.write(response_chat.text)
                    except Exception as e: st.error(f"å°è©±éŒ¯èª¤ï¼š{e}")

    else:
        # --- ä¸€èˆ¬æ¨¡å¼ (Standard RAG) ---
        actual_model = model_selection
        mode = st.radio("ç¿»è­¯æ–¹å‘", ["é˜¿ç¾èª â®• ä¸­æ–‡", "ä¸­æ–‡ â®• é˜¿ç¾èª"], horizontal=True)
        direction = "AtoZ" if mode == "é˜¿ç¾èª â®• ä¸­æ–‡" else "ZtoA"
        if "rag_result" not in st.session_state: st.session_state.rag_result = None
        if "last_query" not in st.session_state: st.session_state.last_query = ""
        st.subheader("è¼¸å…¥æ–‡å­—")
        with st.form("translation_search"):
            q = st.text_area(f"åœ¨æ­¤è¼¸å…¥å¥å­", height=150)
            submit_search = st.form_submit_button("ğŸš€ 1. æŸ¥è©¢èªæ–™åº«", type="primary")
        if submit_search and q:
            f, w, s, r = get_expert_knowledge(q, direction)
            st.session_state.rag_result = (f, w, s, r)
            st.session_state.last_query = q
        st.divider()
        if st.session_state.rag_result:
            f, w, s, r = st.session_state.rag_result
            if f: st.success(f"### ğŸ† å°ˆå®¶ç¿»è­¯ï¼š\n**{f}**")
            if w:
                with st.expander(f"ğŸ“š ç›¸é—œå–®è© ({len(w)} ç­†)", expanded=True):
                    for item in w: st.markdown(f"- **{item['amis']}** â®• {item['chinese']} ({item['pos']})")
            if s:
                with st.expander(f"ğŸ—£ï¸ ç›¸é—œä¾‹å¥ ({len(s)} ç­†)", expanded=True):
                    for item in s: st.markdown(f"> **{item['amis']}**\n> ({item['chinese']})")
            st.divider()
            st.markdown("### ğŸ¤– AI å”åŒåˆ†æ")
            if st.button("ğŸ¦… åŸ·è¡Œ AI èªæ³•åˆ†æ"):
                if not api_key: st.warning("è«‹è¨­å®š API Key")
                else:
                    try:
                        with st.spinner(f"æ­£åœ¨å‘¼å« {actual_model} ..."):
                            genai.configure(api_key=api_key)
                            m = genai.GenerativeModel(actual_model)
                            final_prompt = f"{r}\n\n{missing_word_protocol}\n\nè«‹æ ¹æ“šä»¥ä¸Šæä¾›çš„ã€é˜¿ç¾èªèªæ–™åº«ã€‘ï¼Œå°ä»¥ä¸‹å¥å­é€²è¡Œè©³ç´°èªæ³•èˆ‡èªæ„åˆ†æã€‚\n\nä½¿ç”¨è€…è¼¸å…¥: {st.session_state.last_query}"
                            try:
                                response = m.generate_content(final_prompt)
                            except Exception as e:
                                if "429" in str(e):
                                    time.sleep(60) # ç°¡æ˜“å†·å»
                                    response = m.generate_content(final_prompt)
                                else: raise e

                            if response:
                                st.markdown("#### ğŸ¦… AI åˆ†æå ±å‘Šï¼š")
                                st.write(response.text)
                    except Exception as e: st.error(f"âš ï¸ AI éŒ¯èª¤ï¼š{e}")

# ==========================================
# 3. ä¸»æ§å°
# ==========================================

def main():
    with sqlite3.connect('amis_data.db') as conn:
        conn.execute('CREATE TABLE IF NOT EXISTS sentence_pairs (id INTEGER PRIMARY KEY AUTOINCREMENT, created_at TIMESTAMP, output_sentencepattern_amis TEXT, output_sentencepattern_chinese TEXT, note TEXT)')
        conn.execute('CREATE TABLE IF NOT EXISTS vocabulary (id INTEGER PRIMARY KEY AUTOINCREMENT, amis TEXT, chinese TEXT, english TEXT, part_of_speech TEXT, note TEXT, created_at TIMESTAMP)')
        conn.execute('CREATE TABLE IF NOT EXISTS pos_tags (tag_name TEXT PRIMARY KEY, sort_order INTEGER DEFAULT 0)')
    st.sidebar.title("ğŸ¦… ç³»çµ±é¸å–®")
    
    with st.sidebar.expander("ğŸ“‚ è³‡æ–™åº«æ•‘æ´ä¸­å¿ƒ", expanded=True):
        st.warning("âš ï¸ è­¦å‘Šï¼šè‹¥é›²ç«¯è³‡æ–™éºå¤±ï¼Œè«‹åœ¨æ­¤ä¸Šå‚³æœ¬æ©Ÿå‚™ä»½æª” (.db) é€²è¡Œé‚„åŸã€‚")
        uploaded_db = st.file_uploader("ä¸Šå‚³ amis_data.db", type=["db"])
        if uploaded_db is not None:
            if st.button("ğŸš¨ ç¢ºèªè¦†è“‹ä¸¦é‚„åŸè³‡æ–™åº«"):
                with open("amis_data.db", "wb") as f:
                    f.write(uploaded_db.getbuffer())
                st.success("âœ… è³‡æ–™åº«é‚„åŸæˆåŠŸï¼è«‹é‡æ–°æ•´ç†é é¢ã€‚")
                time.sleep(2)
                st.rerun()

    with st.sidebar.container():
        st.info("â˜ï¸ **è¡Œå‹•åŒæ­¥ä¸­å¿ƒ**")
        if st.sidebar.button("ğŸ”„ ç«‹å³å°‡è³‡æ–™å‚™ä»½å› GitHub", type="primary"):
            backup_to_github()
    
    default_key = st.secrets.get("GOOGLE_API_KEY", "")
    key = st.sidebar.text_input("Google API Key", type="password", value=st.session_state.get("api_key", default_key))
    
    if key != st.session_state.get("api_key"): 
        st.session_state["api_key"] = key; st.cache_resource.clear(); st.rerun()
    
    raw_ms = get_verified_models(key)
    ms = []
    if raw_ms:
        ms = raw_ms.copy()
        DREAM_MODEL = "ğŸ§¬ Pangcah/'Amis_language_mode"
        ms.insert(0, DREAM_MODEL)
    model = st.sidebar.selectbox("è«‹é¸æ“‡ AI æ¨¡å‹", ms, index=0) if ms else None
    
    st.sidebar.divider()
    page = st.sidebar.radio("åŠŸèƒ½æ¨¡å¼", ["ğŸ  ç³»çµ±é¦–é ", "â— AI æ™ºæ…§åŠ©ç†", "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«", "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†", "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†", "ğŸ“ èªæ–™åŒ¯å‡º"])
    
    if page == "ğŸ  ç³»çµ±é¦–é ":
        st.markdown("<h1 style='text-align: center; font-size: 5rem;'>ğŸ¦…</h1>", unsafe_allow_html=True)
        st.markdown("<h1 style='text-align: center;'>'Amis / Pangcah AI</h1>", unsafe_allow_html=True)
        st.divider()
        st.markdown("<p style='text-align: center;'>æ­¡è¿å›ä¾†ï¼Œèˆ¹é•·ã€‚ç³»çµ±å·²å°±ç·’ã€‚</p>", unsafe_allow_html=True)

    elif page == "â— AI æ™ºæ…§åŠ©ç†": assistant_system(key, model)

    elif page == "ğŸ” å¥å‹ï¼šå°ˆå®¶è³‡æ–™åº«":
        st.title("ğŸ” å°ˆå®¶å¥å‹è³‡æ–™åº«")
        with st.form("add_new_s"):
            c1, c2, c3 = st.columns(3)
            a, c, n = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡"), c3.text_input("å‚™è¨»")
            if st.form_submit_button("â• å„²å­˜æ–°å¥å‹"):
                if a and c: 
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO sentence_pairs (output_sentencepattern_amis, output_sentencepattern_chinese, note, created_at) VALUES (?,?,?,?)", (a, c, n, now))
                    sync_vocabulary(a); reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs ORDER BY id DESC", conn)
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic", hide_index=True)
        
        col_save, col_download = st.columns([1, 4])
        with col_save:
            if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
                with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('sentence_pairs', conn, if_exists='replace', index=False)
                reorder_ids("sentence_pairs"); backup_to_github(); st.rerun()
        with col_download:
            csv_data = edited_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel/CSV", csv_data, f'amis_sentences_{datetime.now().strftime("%Y%m%d")}.csv', 'text/csv')

        # --- æ–°å¢å€å¡Šï¼šä¸Šå‚³è¦†è“‹ ---
        st.markdown("---")
        with st.expander("ğŸ“‚ æ‰¹æ¬¡åŒ¯å…¥/é‚„åŸ (ä¸Šå‚³ CSV)", expanded=False):
            st.error("âš ï¸ å±éšªæ“ä½œï¼šä¸Šå‚³ CSV å°‡æœƒã€å®Œå…¨è¦†è“‹ã€‘ä¸¦åˆªé™¤ç¾æœ‰çš„å¥å‹è³‡æ–™ï¼")
            uploaded_csv = st.file_uploader("è«‹é¸æ“‡è¦ä¸Šå‚³çš„ CSV æª” (å¥å‹)", type=["csv"])
            if uploaded_csv is not None:
                if st.button("ğŸš¨ ç¢ºèªè¦†è“‹ä¸¦åŒ¯å…¥å¥å‹", type="primary"):
                    try:
                        df_upload = pd.read_csv(uploaded_csv)
                        # æª¢æŸ¥å¿…è¦æ¬„ä½
                        required = ['output_sentencepattern_amis', 'output_sentencepattern_chinese']
                        if not all(col in df_upload.columns for col in required):
                            st.error(f"âŒ æ ¼å¼éŒ¯èª¤ï¼CSV å¿…é ˆåŒ…å«é€™äº›æ¬„ä½: {required}")
                        else:
                            # è£œé½Šæ¬„ä½
                            if 'note' not in df_upload.columns: df_upload['note'] = ""
                            if 'created_at' not in df_upload.columns: df_upload['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            
                            with sqlite3.connect('amis_data.db') as conn:
                                df_upload.to_sql('sentence_pairs', conn, if_exists='replace', index=False)
                            reorder_ids("sentence_pairs")
                            backup_to_github()
                            st.success(f"âœ… æˆåŠŸåŒ¯å…¥ {len(df_upload)} ç­†å¥å‹ï¼(èˆŠè³‡æ–™å·²è¦†è“‹)")
                            time.sleep(2); st.rerun()
                    except Exception as e:
                        st.error(f"åŒ¯å…¥å¤±æ•—: {e}")

    elif page == "ğŸ“– å–®è©ï¼šèªæ–™åº«ç®¡ç†":
        st.title("ğŸ“– å–®è©èªæ–™åº«ç®¡ç†")
        raw_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
        with st.form("add_new_vocab"):
            c1, c2, c4 = st.columns([2, 2, 3])
            a_in, c_in = c1.text_input("é˜¿ç¾èª"), c2.text_input("ä¸­æ–‡")
            p_in = c4.selectbox("è©é¡", options=raw_tags)
            if st.form_submit_button("â• å„²å­˜æ–°å–®è©"):
                if a_in:
                    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    run_query("INSERT INTO vocabulary (amis, chinese, part_of_speech, created_at) VALUES (?,?,?,?)", (a_in, c_in, p_in, now))
                    reorder_ids("vocabulary"); backup_to_github(); st.rerun()
        st.divider()
        with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM vocabulary ORDER BY id DESC", conn)
        edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic",
            column_config={"part_of_speech": st.column_config.SelectboxColumn("è©é¡ (æœå°‹é¸å–®)", options=raw_tags, required=True)})
        
        col_save, col_download = st.columns([1, 4])
        with col_save:
            if st.button("ğŸ’¾ å„²å­˜ä¿®æ”¹"):
                with sqlite3.connect('amis_data.db') as conn: edited_df.to_sql('vocabulary', conn, if_exists='replace', index=False)
                reorder_ids("vocabulary"); backup_to_github(); st.rerun()
        with col_download:
            csv_data = edited_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel/CSV", csv_data, f'amis_vocabulary_{datetime.now().strftime("%Y%m%d")}.csv', 'text/csv')

        # --- æ–°å¢å€å¡Šï¼šä¸Šå‚³è¦†è“‹ ---
        st.markdown("---")
        with st.expander("ğŸ“‚ æ‰¹æ¬¡åŒ¯å…¥/é‚„åŸ (ä¸Šå‚³ CSV)", expanded=False):
            st.error("âš ï¸ å±éšªæ“ä½œï¼šä¸Šå‚³ CSV å°‡æœƒã€å®Œå…¨è¦†è“‹ã€‘ä¸¦åˆªé™¤ç¾æœ‰çš„å–®è©è³‡æ–™ï¼")
            uploaded_csv_v = st.file_uploader("è«‹é¸æ“‡è¦ä¸Šå‚³çš„ CSV æª” (å–®è©)", type=["csv"])
            if uploaded_csv_v is not None:
                if st.button("ğŸš¨ ç¢ºèªè¦†è“‹ä¸¦åŒ¯å…¥å–®è©", type="primary"):
                    try:
                        df_upload = pd.read_csv(uploaded_csv_v)
                        # æª¢æŸ¥å¿…è¦æ¬„ä½
                        required = ['amis', 'chinese', 'part_of_speech']
                        if not all(col in df_upload.columns for col in required):
                            st.error(f"âŒ æ ¼å¼éŒ¯èª¤ï¼CSV å¿…é ˆåŒ…å«é€™äº›æ¬„ä½: {required}")
                        else:
                            # è£œé½Šæ¬„ä½
                            if 'note' not in df_upload.columns: df_upload['note'] = ""
                            if 'created_at' not in df_upload.columns: df_upload['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            
                            with sqlite3.connect('amis_data.db') as conn:
                                df_upload.to_sql('vocabulary', conn, if_exists='replace', index=False)
                            reorder_ids("vocabulary")
                            backup_to_github()
                            st.success(f"âœ… æˆåŠŸåŒ¯å…¥ {len(df_upload)} ç­†å–®è©ï¼(èˆŠè³‡æ–™å·²è¦†è“‹)")
                            time.sleep(2); st.rerun()
                    except Exception as e:
                        st.error(f"åŒ¯å…¥å¤±æ•—: {e}")

    elif page == "ğŸ·ï¸ èªæ³•æ¨™ç±¤ç®¡ç†":
        st.title("ğŸ·ï¸ æ¨™ç±¤ç®¡ç† (Tag Alignment)")
        with st.expander("âš¡ æ™ºæ…§æ›´åå·¥å…· (é€£å‹•æ›´æ–°å–®è©)", expanded=True):
            current_tags = [r[0] for r in run_query("SELECT tag_name FROM pos_tags", fetch=True) if r[0]]
            c1, c2 = st.columns(2)
            old_tag = c1.selectbox("é¸æ“‡è¦ä¿®æ”¹çš„èˆŠæ¨™ç±¤", options=current_tags)
            new_tag_name = c2.text_input("è¼¸å…¥æ–°åç¨±")
            if st.button("ğŸ”„ åŸ·è¡Œæ›´åèˆ‡é€£å‹•æ›´æ–°"):
                if old_tag and new_tag_name and old_tag != new_tag_name:
                    try:
                        with sqlite3.connect('amis_data.db') as conn:
                            conn.execute("UPDATE vocabulary SET part_of_speech = ? WHERE part_of_speech = ?", (new_tag_name, old_tag))
                            conn.execute("INSERT OR IGNORE INTO pos_tags (tag_name) VALUES (?)", (new_tag_name,))
                            conn.execute("DELETE FROM pos_tags WHERE tag_name = ?", (old_tag,))
                        st.success(f"âœ… æˆåŠŸå°‡ '{old_tag}' æ›´åç‚º '{new_tag_name}'ï¼Œä¸¦æ›´æ–°äº†ç›¸é—œå–®è©ï¼")
                        backup_to_github(); time.sleep(1.5); st.rerun()
                    except Exception as e: st.error(f"æ›´æ–°å¤±æ•—: {e}")
        st.divider()
        with st.form("t"):
            nt = st.text_input("æ–°å¢æ¨™ç±¤åç¨±")
            if st.form_submit_button("æ–°å¢"): 
                run_query("INSERT OR REPLACE INTO pos_tags (tag_name) VALUES (?)", (nt,)) 
                backup_to_github(); st.rerun()
        with sqlite3.connect('amis_data.db') as conn: 
            df_tags = pd.read_sql("SELECT * FROM pos_tags", conn)
        if "description" not in df_tags.columns: df_tags["description"] = "" 
        cols_order = ["tag_name", "description", "sort_order"]
        existing_cols = [c for c in cols_order if c in df_tags.columns]
        remaining_cols = [c for c in df_tags.columns if c not in existing_cols]
        df_tags = df_tags[existing_cols + remaining_cols]
        et = st.data_editor(
            df_tags, 
            use_container_width=True, 
            num_rows="dynamic",
            column_config={
                "tag_name": st.column_config.TextColumn("èªæ³•æ¨™ç±¤åç¨±", disabled=True), 
                "description": st.column_config.TextColumn("å‚™è¨» (LLM å®šç¾©æ ¡æº–)", help="åœ¨æ­¤èªªæ˜æ­¤æ¨™ç±¤èˆ‡å¤§èªè¨€æ¨¡å‹é€šç”¨å®šç¾©çš„å·®ç•°", width="large"),
                "sort_order": st.column_config.NumberColumn("æ’åºæ¬Šé‡")
            }
        )
        if st.button("ğŸ’¾ å„²å­˜æ¨™ç±¤èˆ‡å‚™è¨»"):
            with sqlite3.connect('amis_data.db') as conn: 
                et.to_sql('pos_tags', conn, if_exists='replace', index=False)
            backup_to_github(); st.success("å·²å­˜æª”ï¼è³‡æ–™åº«çµæ§‹å·²è‡ªå‹•æ›´æ–°ã€‚"); st.rerun()

    elif page == "ğŸ“ èªæ–™åŒ¯å‡º":
        st.title("ğŸ“ èªæ–™åŒ¯å‡ºèˆ‡æˆ°ç•¥é€²åº¦")
        with st.container():
            st.info("ğŸ—ºï¸ **AI æˆ°ç•¥ç™¼å±•è·¯ç·šåœ– (Roadmap)**")
            c1, c2, c3 = st.columns(3)
            with c1: st.markdown("### ğŸš© ç¬¬ä¸€éšæ®µ (ç›®å‰)"); st.caption("RAG æª¢ç´¢å¢å¼·ç”Ÿæˆ"); st.write("âœ… **Python æ¡ç¤¦æ©Ÿ**\nâœ… **Gemini å»šå¸«**\nğŸ› ï¸ **ç›®æ¨™**ï¼šæŒçºŒæ“´å……èªæ–™åº«ã€‚")
            with c2: st.markdown("### ğŸ”ï¸ ç¬¬äºŒéšæ®µ (1,000+)"); st.caption("å¾®èª¿ (Fine-tuning)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šåˆæ­¥å»ºç«‹å°ˆå±¬æ¨¡å‹ã€‚")
            with c3: st.markdown("### åŸå ¡ğŸ° ç¬¬ä¸‰éšæ®µ (10,000+)"); st.caption("åŸç”Ÿæ¨¡å‹ (Native LLM)"); st.write("ğŸ› ï¸ **ç›®æ¨™**ï¼šé˜¿ç¾èªåŸç”Ÿæ¨ç†èƒ½åŠ›ã€‚")
        st.divider()
        tab1, tab2 = st.tabs(["ğŸ“ å¥å‹", "ğŸ“– å–®è©"])
        with tab1:
            with sqlite3.connect('amis_data.db') as conn: df = pd.read_sql("SELECT * FROM sentence_pairs", conn)
            st.dataframe(df, use_container_width=True)
            c1, c2 = st.columns(2)
            with c1: st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df.to_json(orient="records", lines=True, force_ascii=False), "amis_sentences.jsonl")
            with c2: st.download_button("ğŸ“Š ä¸‹è¼‰ CSV (Excel)", df.to_csv(index=False).encode('utf-8-sig'), "amis_sentences.csv", "text/csv")
        with tab2:
            with sqlite3.connect('amis_data.db') as conn: df_v = pd.read_sql("SELECT * FROM vocabulary", conn)
            st.dataframe(df_v, use_container_width=True)
            c1, c2 = st.columns(2)
            with c1: st.download_button("ğŸ“¥ ä¸‹è¼‰ JSONL", df_v.to_json(orient="records", lines=True, force_ascii=False), "amis_vocabulary.jsonl")
            with c2: st.download_button("ğŸ“Š ä¸‹è¼‰ CSV (Excel)", df_v.to_csv(index=False).encode('utf-8-sig'), "amis_vocabulary.csv", "text/csv")

if __name__ == "__main__": main()
